{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# æ–‡æœ¬é¢„å¤„ç†\n",
    "\n",
    "**åˆ†ç±»:** é«˜çº§ä¸»é¢˜\n",
    "\n",
    "**ğŸ“– å­—å…¸æŸ¥å­—**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª æƒ³è±¡è¿™æ ·ä¸€ä¸ªåœºæ™¯\n",
    "ç”µè„‘çœ‹ä¸æ‡‚ä¸­æ–‡ï¼Œå®ƒåªè®¤è¯†æ•°å­—ã€‚\n",
    "æˆ‘ä»¬è¦æŠŠã€Šè¥¿æ¸¸è®°ã€‹å–‚ç»™ç”µè„‘åƒã€‚\n",
    "\n",
    "1. **åˆ†è¯**: æŠŠ\"å­™æ‚Ÿç©ºæ‰“å¦–æ€ª\"åˆ‡æˆ [\"å­™æ‚Ÿç©º\", \"æ‰“\", \"å¦–æ€ª\"]ã€‚\n",
    "2. **å»ºç«‹å­—å…¸**: å­™æ‚Ÿç©º=1, æ‰“=2, å¦–æ€ª=3ã€‚\n",
    "3. **ç¼–ç **: å˜æˆ [1, 2, 3]ã€‚\n",
    "\n",
    "è¿™å°±æ˜¯**æ–‡æœ¬é¢„å¤„ç†**ï¼šæŠŠäººç±»è¯­è¨€ç¿»è¯‘æˆç”µè„‘èƒ½æ‡‚çš„æ•°å­—ä»£ç ã€‚\n",
    "\n",
    "### ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªæŠ€æœ¯?\n",
    "\n",
    "**é—®é¢˜:** è®¡ç®—æœºæ— æ³•ç›´æ¥è®¡ç®—æ–‡å­—ã€‚\n",
    "\n",
    "**è§£å†³:** é€šè¿‡åˆ†è¯ã€æ„å»ºè¯è¡¨ã€æ•°å­—åŒ–ç´¢å¼•ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ã€‚\n",
    "\n",
    "### ğŸ“š å¾ªåºæ¸è¿›å­¦ä¹ \n",
    "\n",
    "**ç¬¬ä¸€æ­¥: ç†è§£é—®é¢˜** (ä½ ç°åœ¨åœ¨è¿™é‡Œ)\n",
    "- ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªæŠ€æœ¯?\n",
    "- å®ƒè§£å†³ä»€ä¹ˆé—®é¢˜?\n",
    "\n",
    "**ç¬¬äºŒæ­¥: å­¦ä¹ åŸç†** (æ¥ä¸‹æ¥)\n",
    "- è¿™ä¸ªæŠ€æœ¯å¦‚ä½•å·¥ä½œ?\n",
    "- æ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆ?\n",
    "\n",
    "**ç¬¬ä¸‰æ­¥: å®é™…åº”ç”¨** (æœ€å)\n",
    "- å¦‚ä½•åº”ç”¨åˆ°å®é™…é¡¹ç›®?\n",
    "- å¦‚ä½•è§£å†³å®é™…é—®é¢˜?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "",
    "## ğŸ”° æ–°æ‰‹å¿…çœ‹",
    "",
    "**ç¬¬ä¸€æ¬¡å­¦ï¼Ÿè¿™äº›æç¤ºèƒ½å¸®åˆ°ä½ ï¼**",
    "",
    "### ğŸ’¡ å­¦ä¹ å»ºè®®",
    "",
    "1. **ä¸è¦æ€¥** - æ…¢æ…¢çœ‹ï¼Œä¸æ‡‚çš„å¤šçœ‹å‡ é",
    "2. **åŠ¨æ‰‹åš** - æ¯ä¸ªä»£ç éƒ½è¿è¡Œä¸€é",
    "3. **æ”¹å‚æ•°** - è¯•ç€æ”¹æ”¹æ•°å­—ï¼Œçœ‹çœ‹ä¼šæ€æ ·",
    "4. **è®°ç¬”è®°** - æŠŠé‡ç‚¹è®°ä¸‹æ¥",
    "",
    "### âš ï¸ å¸¸è§é—®é¢˜",
    "",
    "**Q: ä»£ç æŠ¥é”™æ€ä¹ˆåŠï¼Ÿ**",
    "- å…ˆçœ‹é”™è¯¯æç¤ºï¼ˆçº¢è‰²çš„é‚£è¡Œï¼‰",
    "- æ£€æŸ¥æ˜¯å¦æœ‰æ‹¼å†™é”™è¯¯",
    "- ç¡®è®¤ç¼©è¿›æ˜¯å¦æ­£ç¡®ï¼ˆPythonå¯¹ç©ºæ ¼å¾ˆæ•æ„Ÿï¼‰",
    "- å¤åˆ¶é”™è¯¯ä¿¡æ¯æœç´¢ä¸€ä¸‹",
    "",
    "**Q: çœ‹ä¸æ‡‚æ€ä¹ˆåŠï¼Ÿ**",
    "- è·³è¿‡éš¾çš„éƒ¨åˆ†ï¼Œå…ˆå­¦ç®€å•çš„",
    "- çœ‹çœ‹å‰é¢çš„è¯¾ç¨‹æœ‰æ²¡æœ‰é—æ¼",
    "- å¤šçœ‹å‡ éï¼Œç†è§£éœ€è¦æ—¶é—´",
    "",
    "**Q: éœ€è¦ä»€ä¹ˆåŸºç¡€ï¼Ÿ**",
    "- ä¼šç”¨ç”µè„‘å°±è¡Œ",
    "- PythonåŸºç¡€æœ€å¥½æœ‰ï¼Œæ²¡æœ‰ä¹Ÿèƒ½å­¦",
    "- æ•°å­¦ä¸å¥½ä¹Ÿæ²¡å…³ç³»ï¼Œæˆ‘ä»¬ç”¨ä¾‹å­è®²",
    "",
    "### ğŸ“Œ å­¦ä¹ æŠ€å·§",
    "",
    "- ğŸ¯ **ç›®æ ‡æ˜ç¡®**: çŸ¥é“è¿™èŠ‚è¯¾è¦å­¦ä»€ä¹ˆ",
    "- ğŸ“ **åšç¬”è®°**: é‡ç‚¹å†…å®¹è®°ä¸‹æ¥",
    "- ğŸ’» **å¤šç»ƒä¹ **: ä»£ç è¦è‡ªå·±æ•²ä¸€é",
    "- ğŸ¤” **å¤šæ€è€ƒ**: æƒ³æƒ³ä¸ºä»€ä¹ˆè¿™æ ·åš",
    "- ğŸ”„ **å¤šå¤ä¹ **: å­¦å®Œäº†å›å¤´å†çœ‹çœ‹",
    "",
    "---",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ–‡æœ¬é¢„å¤„ç†\n",
    "\n",
    "**åˆ†ç±»:** æ•°æ®å¤„ç†\n",
    "\n",
    "**ğŸ“Š æ•°æ®æ˜¯AIçš„ç‡ƒæ–™,å­¦ä¼šå¤„ç†æ•°æ®æ˜¯ç¬¬ä¸€æ­¥**\n",
    "\n",
    "---\n",
    "\n",
    "\n## ğŸ’­ å¼€å§‹ä¹‹å‰:æƒ³æƒ³è¿™ä¸ªé—®é¢˜\n\nå­¦ä¹  **æ–‡æœ¬é¢„å¤„ç†** èƒ½å¸®æˆ‘ä»¬è§£å†³ä»€ä¹ˆå®é™…é—®é¢˜?\n\nåœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­,æ•°æ®æ— å¤„ä¸åœ¨:\n- ğŸ“± æ‰‹æœºAppéœ€è¦å¤„ç†ç”¨æˆ·æ•°æ®\n- ğŸ® æ¸¸æˆéœ€è¦å¤„ç†ç©å®¶æ•°æ®\n- ğŸ›’ ç½‘è´­éœ€è¦å¤„ç†å•†å“æ•°æ®\n- ğŸ“º è§†é¢‘æ¨èéœ€è¦å¤„ç†è§‚çœ‹æ•°æ®\n\n**æ•°æ®å°±åƒAIçš„\"ç‡ƒæ–™\",æ²¡æœ‰æ•°æ®,AIå°±æ— æ³•å·¥ä½œ!**\n\n### ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦å¤„ç†æ•°æ®?\n\n**é—®é¢˜1: æ•°æ®æ ¼å¼ä¸ç»Ÿä¸€**\n- æœ‰äº›æ•°æ®æ˜¯å›¾ç‰‡,æœ‰äº›æ˜¯æ–‡æœ¬\n- æœ‰äº›æ•°æ®å¾ˆå¤§,æœ‰äº›å¾ˆå°\n- éœ€è¦ç»Ÿä¸€æ ¼å¼\n\n**é—®é¢˜2: æ•°æ®è´¨é‡ä¸å¥½**\n- æœ‰äº›æ•°æ®æœ‰é”™è¯¯\n- æœ‰äº›æ•°æ®ä¸å®Œæ•´\n- éœ€è¦æ¸…æ´—æ•°æ®\n\n**é—®é¢˜3: æ•°æ®å¤ªå¤š**\n- æ•°æ®é‡å¤ªå¤§,å†…å­˜è£…ä¸ä¸‹\n- éœ€è¦æ‰¹é‡å¤„ç†\n- éœ€è¦é«˜æ•ˆåŠ è½½\n\n### ğŸ“š å¾ªåºæ¸è¿›å­¦ä¹ \n\n**ç¬¬ä¸€æ­¥: ç†è§£é—®é¢˜** (ä½ ç°åœ¨åœ¨è¿™é‡Œ)\n- ä¸ºä»€ä¹ˆéœ€è¦å¤„ç†æ•°æ®?\n- æ•°æ®æœ‰ä»€ä¹ˆé—®é¢˜?\n\n**ç¬¬äºŒæ­¥: å­¦ä¹ åŸºæœ¬æ“ä½œ** (æ¥ä¸‹æ¥)\n- å¦‚ä½•åŠ è½½æ•°æ®?\n- å¦‚ä½•é¢„å¤„ç†æ•°æ®?\n\n**ç¬¬ä¸‰æ­¥: å®é™…åº”ç”¨** (æœ€å)\n- å¦‚ä½•åº”ç”¨åˆ°å®é™…é¡¹ç›®?\n- å¦‚ä½•ä¼˜åŒ–æ€§èƒ½?\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ æœ¬èŠ‚è¯¾ä½ å°†å­¦ä¼š\n",
    "\n",
    "- âœ… ç†è§£æ ¸å¿ƒæ¦‚å¿µå’ŒåŸç†\n",
    "- âœ… æŒæ¡å®é™…ä»£ç å®ç°\n",
    "- âœ… çŸ¥é“å¦‚ä½•åº”ç”¨åˆ°å®é™…é¡¹ç›®\n",
    "- âœ… ç†è§£è¿™ä¸ªæŠ€æœ¯è§£å†³ä»€ä¹ˆé—®é¢˜\n",
    "\n",
    "## ğŸ’¡ å­¦ä¹ å»ºè®®\n",
    "\n",
    "1. **å…ˆç†è§£\"ä¸ºä»€ä¹ˆ\"** - è¿™ä¸ªæŠ€æœ¯è§£å†³ä»€ä¹ˆå®é™…é—®é¢˜?\n",
    "2. **å†å­¦ä¹ \"æ˜¯ä»€ä¹ˆ\"** - è¿™ä¸ªæŠ€æœ¯çš„åŸç†æ˜¯ä»€ä¹ˆ?\n",
    "3. **æœ€åæŒæ¡\"æ€ä¹ˆåš\"** - å¦‚ä½•ç”¨ä»£ç å®ç°?\n",
    "4. **åŠ¨æ‰‹å®è·µ** - è¿è¡Œä»£ç ,ä¿®æ”¹å‚æ•°,è§‚å¯Ÿç»“æœ\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ–‡æœ¬é¢„å¤„ç†\n",
    "\n",
    "**åˆ†ç±»:** æ•°æ®å¤„ç†\n",
    "\n",
    "**ğŸ¯ è®©æˆ‘ä»¬ä»å®é™…é—®é¢˜å‡ºå‘**\n",
    "\n",
    "---\n",
    "\n",
    "\n## ğŸ’­ å¼€å§‹ä¹‹å‰,æƒ³æƒ³è¿™ä¸ªé—®é¢˜\n\nå­¦ä¹  **æ–‡æœ¬é¢„å¤„ç†** èƒ½å¸®æˆ‘ä»¬è§£å†³ä»€ä¹ˆå®é™…é—®é¢˜?\n\nåœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­,ä½ å¯èƒ½å·²ç»åœ¨ä¸çŸ¥ä¸è§‰ä¸­ä½¿ç”¨äº†è¿™ä¸ªæŠ€æœ¯:\n- ğŸ“± æ‰‹æœºApp\n- ğŸ® æ¸¸æˆ\n- ğŸ›’ ç½‘è´­\n- ğŸ“º è§†é¢‘æ¨è\n\nè®©æˆ‘ä»¬ä¸€èµ·æ¢ç´¢è¿™ä¸ªæŠ€æœ¯èƒŒåçš„åŸç†!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ æœ¬èŠ‚è¯¾ä½ å°†å­¦ä¼š\n",
    "\n",
    "- âœ… ç†è§£æ ¸å¿ƒæ¦‚å¿µå’ŒåŸç†\n",
    "- âœ… æŒæ¡å®é™…ä»£ç å®ç°\n",
    "- âœ… çŸ¥é“å¦‚ä½•åº”ç”¨åˆ°å®é™…é¡¹ç›®\n",
    "- âœ… ç†è§£è¿™ä¸ªæŠ€æœ¯è§£å†³ä»€ä¹ˆé—®é¢˜\n",
    "\n",
    "## ğŸ’¡ å­¦ä¹ å»ºè®®\n",
    "\n",
    "1. **å…ˆç†è§£\"ä¸ºä»€ä¹ˆ\"** - è¿™ä¸ªæŠ€æœ¯è§£å†³ä»€ä¹ˆå®é™…é—®é¢˜?\n",
    "2. **å†å­¦ä¹ \"æ˜¯ä»€ä¹ˆ\"** - è¿™ä¸ªæŠ€æœ¯çš„åŸç†æ˜¯ä»€ä¹ˆ?\n",
    "3. **æœ€åæŒæ¡\"æ€ä¹ˆåš\"** - å¦‚ä½•ç”¨ä»£ç å®ç°?\n",
    "4. **åŠ¨æ‰‹å®è·µ** - è¿è¡Œä»£ç ,ä¿®æ”¹å‚æ•°,è§‚å¯Ÿç»“æœ\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. æ–‡æœ¬é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â‘  å°†æ•°æ®é›†è¯»å–åˆ°ç”±æ–‡æœ¬è¡Œç»„æˆçš„åˆ—è¡¨ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the time machine by h g wells\n",
      "twinkled and his usually pale face was flushed and animated the\n"
     ]
    }
   ],
   "source": [
    "# ä¸‹è½½å¹¶å­˜å‚¨ 'time_machine' æ•°æ®é›†çš„ URL å’Œå“ˆå¸Œå€¼\n",
    "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',\n",
    "                               '090b5e7e70c295757f55df93cb0a180b9691891a')  \n",
    "\n",
    "def read_time_machine():\n",
    "    \"\"\"Load the time machine dataset into a list of text lines. \"\"\"\n",
    "    \"\"\"å°†æ—¶é—´æœºå™¨æ•°æ®é›†åŠ è½½ä¸ºæ–‡æœ¬è¡Œçš„åˆ—è¡¨ã€‚\"\"\"\n",
    "    # æ‰“å¼€ 'time_machine' æ•°æ®é›†æ–‡ä»¶ï¼Œå¹¶ä½¿ç”¨æ–‡ä»¶å¯¹è±¡ f è¿›è¡Œæ“ä½œ\n",
    "    with open(d2l.download('time_machine'), 'r') as f:\n",
    "        # è¯»å–æ–‡ä»¶çš„æ‰€æœ‰è¡Œï¼Œå¹¶å°†æ¯è¡Œå­˜å‚¨åœ¨åˆ—è¡¨ lines ä¸­\n",
    "        lines = f.readlines()\n",
    "        # æŠŠä¸æ˜¯å¤§å†™å­—æ¯ã€å°å†™å­—æ¯çš„ä¸œè¥¿ï¼Œå…¨éƒ¨å˜æˆç©ºæ ¼\n",
    "        # å»é™¤éå­—æ¯å­—ç¬¦ï¼Œå¹¶è½¬æ¢ä¸ºå°å†™\n",
    "    return [re.sub('[^A-Za-z]+',' ',line).strip().lower() for line in lines]   \n",
    "\n",
    "# è¯»å–æ—¶é—´æœºå™¨æ•°æ®é›†ï¼Œå¹¶å°†ç»“æœå­˜å‚¨åœ¨ 'lines' å˜é‡ä¸­\n",
    "lines = read_time_machine()\n",
    "# æ‰“å°æ•°æ®é›†çš„ç¬¬ä¸€è¡Œ\n",
    "print(lines[0])\n",
    "# æ‰“å°æ•°æ®é›†çš„ç¬¬11è¡Œï¼ˆç´¢å¼•ä¸º10ï¼‰\n",
    "print(lines[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â‘¡ æ¯ä¸ªæ–‡æœ¬åºåˆ—åˆè¢«æ‹†åˆ†æˆä¸€ä¸ªæ ‡è®°åˆ—è¡¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['i']\n",
      "[]\n",
      "[]\n",
      "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    \"\"\"\n",
    "    å°†æ–‡æœ¬è¡Œåˆ—è¡¨è¿›è¡Œåˆ†è¯å¤„ç†ã€‚\n",
    "\n",
    "    Parameters:\n",
    "        lines (list): æ–‡æœ¬è¡Œåˆ—è¡¨ã€‚\n",
    "        token (str): ä»¤ç‰Œç±»å‹ï¼Œå¯é€‰å€¼ä¸º 'word'ï¼ˆé»˜è®¤ï¼‰æˆ– 'char'ã€‚\n",
    "\n",
    "    Returns:\n",
    "        list: åˆ†è¯åçš„ç»“æœåˆ—è¡¨ã€‚\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # å¦‚æœä»¤ç‰Œç±»å‹ä¸º 'word'\n",
    "    if token == 'word':\n",
    "        # ä»¥ç©ºæ ¼ä¸ºåˆ†éš”ç¬¦å°†æ¯è¡Œå­—ç¬¦ä¸²æ‹†åˆ†ä¸ºå•è¯åˆ—è¡¨\n",
    "        return [line.split() for line in lines]\n",
    "    # å¦‚æœä»¤ç‰Œç±»å‹ä¸º 'char'\n",
    "    elif token == 'char':\n",
    "        # å°†æ¯è¡Œå­—ç¬¦ä¸²æ‹†åˆ†ä¸ºå­—ç¬¦åˆ—è¡¨\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        # è‹¥æŒ‡å®šçš„ä»¤ç‰Œç±»å‹æ— æ•ˆï¼Œåˆ™æ‰“å°é”™è¯¯ä¿¡æ¯\n",
    "        print('é”™ä½ï¼šæœªçŸ¥ä»¤ç‰Œç±»å‹ï¼š' + token)\n",
    "# å¯¹ lines è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œä½¿ç”¨é»˜è®¤çš„ 'word' ä»¤ç‰Œç±»å‹        \n",
    "tokens = tokenize(lines)\n",
    "# æ‰“å°å‰11è¡Œçš„åˆ†è¯ç»“æœ\n",
    "for i in range(11):\n",
    "    # ç©ºåˆ—è¡¨è¡¨ç¤ºç©ºè¡Œ\n",
    "    print(tokens[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â‘¢ æ„å»ºä¸€ä¸ªå­—å…¸ï¼Œé€šå¸¸ä¹Ÿå«åšè¯è¡¨ï¼ˆvocabularyï¼‰ï¼Œç”¨æ¥ä½ å°†å­—ç¬¦ä¸²æ ‡è®°æ˜ å°„åˆ°ä»0å¼€å§‹çš„æ•°å­—ç´¢å¼•ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"æ–‡æœ¬è¯è¡¨\"\"\"\n",
    "    def __init__(self, tokens=None,min_freq=0,reserved_tokens=None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–è¯è¡¨å¯¹è±¡ã€‚\n",
    "\n",
    "        Parameters:\n",
    "            tokens (list): æ ‡è®°åˆ—è¡¨ï¼ˆé»˜è®¤ä¸º Noneï¼‰ã€‚\n",
    "            min_freq (int): æœ€å°é¢‘ç‡é˜ˆå€¼ï¼Œä½äºè¯¥é¢‘ç‡çš„æ ‡è®°å°†è¢«è¿‡æ»¤æ‰ï¼ˆé»˜è®¤ä¸º 0ï¼‰ã€‚\n",
    "            reserved_tokens (list): ä¿ç•™çš„ç‰¹æ®Šæ ‡è®°åˆ—è¡¨ï¼ˆé»˜è®¤ä¸º Noneï¼‰ã€‚\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # å¦‚æœè¾“å…¥çš„ tokens ä¸º Noneï¼Œåˆ™å°†å…¶è®¾ç½®ä¸ºç©ºåˆ—è¡¨\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        # å¦‚æœä¿ç•™çš„ç‰¹æ®Šæ ‡è®°åˆ—è¡¨ reserved_tokens ä¸º Noneï¼Œåˆ™å°†å…¶è®¾ç½®ä¸ºç©ºåˆ—è¡¨\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # ç»Ÿè®¡ tokens ä¸­æ ‡è®°çš„é¢‘ç‡ï¼Œå¹¶è¿”å›ä¸€ä¸ªåŒ…å«æ ‡è®°é¢‘ç‡çš„ Counter å¯¹è±¡\n",
    "        counter = count_corpus(tokens) # éå†å¾—åˆ°æ¯ä¸€ä¸ªç‹¬ä¸€æ— äºŒtokenå‡ºç°çš„æ¬¡æ•°\n",
    "        # æ ¹æ®æ ‡è®°çš„é¢‘ç‡è¿›è¡Œæ’åºï¼Œå¹¶å°†ç»“æœå­˜å‚¨åœ¨ self.token_freqs ä¸­\n",
    "        # sorted() å‡½æ•°ä½¿ç”¨ counter.items() ä½œä¸ºæ’åºå¯¹è±¡ï¼Œä½¿ç”¨æ ‡è®°é¢‘ç‡ x[1] ä½œä¸ºæ’åºä¾æ®ï¼Œé™åºæ’åº\n",
    "        self.token_freqs = sorted(counter.items(),key=lambda x:x[1],reverse=True)\n",
    "        # è®¾ç½®æœªçŸ¥æ ‡è®°ç´¢å¼•ä¸º 0ï¼Œæ„å»ºåŒ…å«æœªçŸ¥æ ‡è®°å’Œä¿ç•™ç‰¹æ®Šæ ‡è®°çš„åˆ—è¡¨ uniq_tokens\n",
    "        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens \n",
    "        # å°†é¢‘ç‡å¤§äºç­‰äº min_freq ä¸”ä¸åœ¨ uniq_tokens ä¸­çš„æ ‡è®°æ·»åŠ åˆ° uniq_tokens åˆ—è¡¨ä¸­\n",
    "        uniq_tokens += [token for token, freq in self.token_freqs\n",
    "                       if freq >= min_freq and token not in uniq_tokens]\n",
    "        # åˆå§‹åŒ–ç´¢å¼•åˆ°æ ‡è®°å’Œæ ‡è®°åˆ°ç´¢å¼•çš„ç©ºåˆ—è¡¨å’Œå­—å…¸\n",
    "        self.idx_to_token, self.token_to_idx = [], dict()\n",
    "        # éå† uniq_tokens ä¸­çš„æ¯ä¸ªæ ‡è®°ï¼Œå°†å…¶æ·»åŠ åˆ°ç´¢å¼•åˆ°æ ‡è®°çš„åˆ—è¡¨ä¸­ï¼Œå¹¶å°†æ ‡è®°å’Œå¯¹åº”ç´¢å¼•å­˜å‚¨åˆ°æ ‡è®°åˆ°ç´¢å¼•çš„å­—å…¸ä¸­\n",
    "        # ç´¢å¼•å€¼ä» 0 å¼€å§‹é€’å¢ï¼Œå¯¹åº”äºæ ‡è®°åœ¨åˆ—è¡¨ä¸­çš„ä½ç½®\n",
    "        for token in uniq_tokens:\n",
    "            # å°†å½“å‰æ ‡è®° `token` æ·»åŠ åˆ°ç´¢å¼•åˆ°æ ‡è®°çš„åˆ—è¡¨ `self.idx_to_token` çš„æœ«å°¾\n",
    "            self.idx_to_token.append(token)\n",
    "            # å°†å½“å‰æ ‡è®° `token` å’Œå…¶å¯¹åº”çš„ç´¢å¼•å€¼å­˜å‚¨åˆ°æ ‡è®°åˆ°ç´¢å¼•çš„å­—å…¸ `self.token_to_idx` ä¸­\n",
    "            # ç´¢å¼•å€¼æ˜¯ `self.idx_to_token` åˆ—è¡¨çš„é•¿åº¦å‡å» 1ï¼Œå³æ ‡è®°åœ¨åˆ—è¡¨ä¸­çš„ä½ç½®ç´¢å¼•\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        è·å–è¯è¡¨çš„é•¿åº¦ã€‚\n",
    "\n",
    "        Parameters:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            int: è¯è¡¨çš„é•¿åº¦ã€‚\n",
    "\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # è·å–è¯è¡¨çš„é•¿åº¦\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        \"\"\"\n",
    "        æ ¹æ®æ ‡è®°è·å–å…¶å¯¹åº”çš„ç´¢å¼•æˆ–ç´¢å¼•åˆ—è¡¨ã€‚\n",
    "\n",
    "        Parameters:\n",
    "            tokens (str or list): æ ‡è®°å­—ç¬¦ä¸²æˆ–æ ‡è®°åˆ—è¡¨ã€‚\n",
    "\n",
    "        Returns:\n",
    "            int or list: æ ‡è®°çš„ç´¢å¼•æˆ–ç´¢å¼•åˆ—è¡¨ã€‚\n",
    "\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # å¦‚æœ tokens ä¸æ˜¯åˆ—è¡¨æˆ–å…ƒç»„ï¼Œåˆ™è¿”å›å¯¹åº”çš„ç´¢å¼•æˆ–é»˜è®¤çš„æœªçŸ¥æ ‡è®°ç´¢å¼•\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        # å¯¹äºè¾“å…¥çš„æ ‡è®°åˆ—è¡¨ tokensï¼Œé€ä¸ªè°ƒç”¨ self.__getitem__() æ–¹æ³•è·å–æ¯ä¸ªæ ‡è®°å¯¹åº”çš„ç´¢å¼•å€¼ï¼Œå¹¶è¿”å›ç´¢å¼•å€¼çš„åˆ—è¡¨  \n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        \"\"\"\n",
    "        æ ¹æ®ç´¢å¼•è·å–å¯¹åº”çš„æ ‡è®°æˆ–æ ‡è®°åˆ—è¡¨ã€‚\n",
    "\n",
    "        Parameters:\n",
    "            indices (int or list): ç´¢å¼•æˆ–ç´¢å¼•åˆ—è¡¨ã€‚\n",
    "\n",
    "        Returns:\n",
    "            str or list: ç´¢å¼•å¯¹åº”çš„æ ‡è®°æˆ–æ ‡è®°åˆ—è¡¨ã€‚\n",
    "\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # å¦‚æœè¾“å…¥çš„ indices ä¸æ˜¯åˆ—è¡¨æˆ–å…ƒç»„ç±»å‹ï¼Œåˆ™è¿”å›å¯¹åº”ç´¢å¼•å€¼å¤„çš„æ ‡è®°\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        # å¯¹äºè¾“å…¥çš„ç´¢å¼•åˆ—è¡¨ indicesï¼Œé€ä¸ªå–å‡ºæ¯ä¸ªç´¢å¼•å€¼ indexï¼Œå¹¶é€šè¿‡ self.idx_to_token[index] è·å–å¯¹åº”çš„æ ‡è®°å€¼ï¼Œæœ€åè¿”å›æ ‡è®°å€¼ç»„æˆçš„åˆ—è¡¨\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "    \n",
    "def count_corpus(tokens):\n",
    "    \"\"\"\n",
    "    ç»Ÿè®¡æ ‡è®°çš„é¢‘ç‡ã€‚\n",
    "\n",
    "    Parameters:\n",
    "        tokens (list): æ ‡è®°åˆ—è¡¨ã€‚\n",
    "\n",
    "    Returns:\n",
    "        collections.Counter: åŒ…å«æ ‡è®°é¢‘ç‡çš„ Counter å¯¹è±¡ã€‚\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # æ£€æŸ¥ tokens æ˜¯å¦æ˜¯ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # å¦‚æœ tokens æ˜¯ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨ï¼Œåˆ™å°†å…¶å±•å¹³ä¸ºä¸€ç»´åˆ—è¡¨\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    # ä½¿ç”¨ collections.Counter ç»Ÿè®¡æ ‡è®°çš„é¢‘ç‡\n",
    "    return collections.Counter(tokens) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â‘£ æ„å»ºè¯æ±‡è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºä¸€ä¸ª Vocab å¯¹è±¡ï¼Œå°†æ ‡è®°åˆ—è¡¨ tokens ä½œä¸ºå‚æ•°ä¼ å…¥ï¼Œç”¨äºæ„å»ºè¯è¡¨\n",
    "vocab = Vocab(tokens)\n",
    "# è·å–è¯è¡¨ä¸­çš„å‰ 10 ä¸ªæ ‡è®°åŠå…¶å¯¹åº”çš„ç´¢å¼•å€¼ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨è¿›è¡Œæ‰“å°è¾“å‡º\n",
    "print(list(vocab.token_to_idx.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â‘£ å°†æ¯ä¸€è¡Œæ–‡æœ¬è½¬æ¢æˆä¸€ä¸ªæ•°å­—ç´¢å¼•åˆ—è¡¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "indices: [1, 19, 50, 40, 2183, 2184, 400]\n",
      "word: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
      "indices: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n"
     ]
    }
   ],
   "source": [
    "# éå†ç´¢å¼•åˆ—è¡¨ [0, 10]\n",
    "for i in [0,10]:\n",
    "    # æ‰“å°å½“å‰ç´¢å¼• i å¤„çš„æ ‡è®°ï¼ˆå•è¯ï¼‰\n",
    "    print('word:', tokens[i])\n",
    "    # è·å–å½“å‰æ ‡è®°åœ¨è¯è¡¨ä¸­çš„ç´¢å¼•å€¼\n",
    "    # æ‰“å°å½“å‰æ ‡è®°åœ¨è¯è¡¨ä¸­çš„ç´¢å¼•å€¼ï¼ˆå¯¹åº”çš„ç´¢å¼•å€¼æˆ–æœªçŸ¥æ ‡è®°ç´¢å¼•ï¼‰\n",
    "    print('indices:',vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â‘¤ å°†æ‰€æœ‰å†…å®¹æ‰“åŒ…åˆ°load_corpus_time_machineå‡½æ•°ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170580, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_corpus_time_machine(max_tokens=-1):\n",
    "    \"\"\"è¿”å›æ—¶å…‰æœºå™¨æ•°æ®é›†çš„æ ‡è®°ç´¢å¼•åˆ—è¡¨å’Œè¯æ±‡è¡¨\"\"\"\n",
    "    # åŠ è½½æ—¶å…‰æœºå™¨æ•°æ®é›†çš„æ–‡æœ¬è¡Œ\n",
    "    lines = read_time_machine()\n",
    "    # å°†æ–‡æœ¬è¡Œè½¬æ¢ä¸ºå­—ç¬¦æ ‡è®°åˆ—è¡¨\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    # æ„å»ºè¯æ±‡è¡¨\n",
    "    vocab = Vocab(tokens)\n",
    "    # å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®°ç´¢å¼•åˆ—è¡¨\n",
    "    corpus = [vocab[token] for line in tokens for token in line]  \n",
    "    # æˆªæ–­æ–‡æœ¬é•¿åº¦ï¼ˆè‹¥æœ‰é™åˆ¶ï¼‰\n",
    "    if max_tokens > 0:\n",
    "        # å¦‚æœè®¾ç½®äº†æœ€å¤§æ ‡è®°æ•° max_tokensï¼Œå¯¹æ ‡è®°ç´¢å¼•åˆ—è¡¨ corpus è¿›è¡Œæˆªæ–­ï¼Œåªä¿ç•™å‰ max_tokens ä¸ªæ ‡è®°\n",
    "        corpus = corpus[:max_tokens]\n",
    "    # è¿”å›æˆªæ–­åçš„æ ‡è®°ç´¢å¼•åˆ—è¡¨ corpus å’Œè¯æ±‡è¡¨ vocab\n",
    "    return corpus, vocab\n",
    "\n",
    "# è½½å…¥æ—¶å…‰æœºå™¨æ•°æ®é›†çš„æ ‡è®°ç´¢å¼•åˆ—è¡¨å’Œè¯æ±‡è¡¨\n",
    "corpus, vocab = load_corpus_time_machine()\n",
    "# æ‰“å°æ—¶å…‰æœºå™¨æ•°æ®é›†çš„æ ‡è®°æ•°å’Œè¯æ±‡è¡¨å¤§å°\n",
    "len(corpus), len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n### ğŸŒ å®é™…åº”ç”¨åœºæ™¯\n\n1. **å›¾åƒè¯†åˆ«ç³»ç»Ÿ**\n   - åœºæ™¯: åŒ»é™¢çš„CTæ‰«æå›¾åƒè¯Šæ–­ç³»ç»Ÿ\n   - åº”ç”¨: æ¯å¤©éœ€è¦åŠ è½½å’Œå¤„ç†æˆåƒä¸Šä¸‡å¼ åŒ»å­¦å½±åƒ\n   - æ•ˆæœ: å¸®åŠ©åŒ»ç”Ÿå¿«é€Ÿå‘ç°è‚ºç»“èŠ‚ã€éª¨æŠ˜ç­‰ç—…å˜\n   - æ¡ˆä¾‹: æŸä¸‰ç”²åŒ»é™¢ä½¿ç”¨AIè¾…åŠ©è¯Šæ–­,å‡†ç¡®ç‡è¾¾95%\n\n2. **è‡ªåŠ¨é©¾é©¶è®­ç»ƒ**\n   - åœºæ™¯: ç‰¹æ–¯æ‹‰ã€ç™¾åº¦Apolloç­‰è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿ\n   - åº”ç”¨: éœ€è¦åŠ è½½æ•°ç™¾ä¸‡å¼ é“è·¯å›¾ç‰‡è¿›è¡Œè®­ç»ƒ\n   - æ•ˆæœ: è®©AIå­¦ä¼šè¯†åˆ«è½¦è¾†ã€è¡Œäººã€äº¤é€šæ ‡å¿—\n   - æ¡ˆä¾‹: Waymoå·²ç»åœ¨ç¾å›½å¤šä¸ªåŸå¸‚æä¾›æ— äººå‡ºç§Ÿè½¦æœåŠ¡\n\n3. **äººè„¸è¯†åˆ«é—¨ç¦**\n   - åœºæ™¯: å°åŒºé—¨ç¦ã€å…¬å¸è€ƒå‹¤ç³»ç»Ÿ\n   - åº”ç”¨: å®æ—¶åŠ è½½å’Œæ¯”å¯¹äººè„¸æ•°æ®\n   - æ•ˆæœ: åˆ·è„¸å¼€é—¨,0.3ç§’è¯†åˆ«,å‡†ç¡®ç‡99.9%\n   - æ¡ˆä¾‹: å…¨å›½å·²æœ‰è¶…è¿‡10ä¸‡ä¸ªå°åŒºä½¿ç”¨äººè„¸è¯†åˆ«é—¨ç¦\n\n4. **çŸ­è§†é¢‘æ¨è**\n   - åœºæ™¯: æŠ–éŸ³ã€å¿«æ‰‹ç­‰çŸ­è§†é¢‘å¹³å°\n   - åº”ç”¨: æ¯ç§’åŠ è½½å¤„ç†æ•°ç™¾ä¸‡ä¸ªè§†é¢‘å¸§\n   - æ•ˆæœ: åˆ†æä½ çš„å–œå¥½,æ¨èä½ å¯èƒ½å–œæ¬¢çš„è§†é¢‘\n   - æ¡ˆä¾‹: æŠ–éŸ³æ—¥æ´»ç”¨æˆ·è¶…6äº¿,æ¯å¤©æ¨èæ•°åäº¿æ¬¡\n\n5. **ç”µå•†å•†å“è¯†åˆ«**\n   - åœºæ™¯: æ·˜å®æ‹ç…§æœåŒæ¬¾åŠŸèƒ½\n   - åº”ç”¨: å¿«é€ŸåŠ è½½å’Œæ¯”å¯¹å•†å“å›¾ç‰‡æ•°æ®åº“\n   - æ•ˆæœ: æ‹å¼ ç…§ç‰‡å°±èƒ½æ‰¾åˆ°åŒæ¬¾æˆ–ç›¸ä¼¼å•†å“\n   - æ¡ˆä¾‹: æ·˜å®æ¯å¤©å¤„ç†è¶…è¿‡1äº¿æ¬¡æ‹ç…§æœç´¢\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š æœ¬èŠ‚å°ç»“\n",
    "\n",
    "æ­å–œä½ å®Œæˆäº†æœ¬èŠ‚å­¦ä¹ !è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹:\n",
    "\n",
    "### âœ… ä½ å­¦åˆ°äº†ä»€ä¹ˆ?\n",
    "- è¯·åœ¨è¿™é‡Œå†™ä¸‹ä½ çš„æ”¶è·...\n",
    "\n",
    "### ğŸ¤” è¿˜æœ‰ç–‘é—®?\n",
    "- è¯·è®°å½•ä¸‹ä½ ä¸ç†è§£çš„åœ°æ–¹...\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥\n",
    "- ç»§ç»­å­¦ä¹ ç›¸å…³ä¸»é¢˜\n",
    "- å°è¯•åšä¸€äº›ç»ƒä¹ é¢˜\n",
    "- åº”ç”¨åˆ°å®é™…é¡¹ç›®ä¸­\n",
    "\n",
    "---\n",
    "\n",
    "**è®°ä½:** å­¦ä¹ æ˜¯ä¸€ä¸ªå¾ªåºæ¸è¿›çš„è¿‡ç¨‹,ä¸è¦ç€æ€¥,æ…¢æ…¢æ¥! ğŸ’ª\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.3",
   "language": "python",
   "name": "python3.6.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "2041.23px",
    "left": "45.9792px",
    "top": "56px",
    "width": "301.875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}