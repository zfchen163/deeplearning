{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 课程5_第2周_特征向量表征\n",
    "\n",
    "**分类:** 吴恩达深度学习课程\n",
    "\n",
    "**🗺️ 国王 - 男人 + 女人 = ?**\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 想象这样一个场景\n",
    "\n",
    "电脑不懂\"苹果\"和\"橘子\"是亲戚。\n",
    "但如果我们把它们画在地图上：\n",
    "- \"苹果\"和\"橘子\"离得很近（都是水果）。\n",
    "- \"苹果\"和\"汽车\"离得很远。\n",
    "\n",
    "这就是**词向量 (Word Embeddings)**。\n",
    "神奇的是，如果你在地图上从\"男人\"走到\"国王\"，同样的距离和方向，如果你从\"女人\"出发，就会走到\"女王\"！\n",
    "\n",
    "### 🎯 为什么需要这个技术?\n",
    "\n",
    "**问题:** One-hot编码无法体现词与词之间的相似性。\n",
    "\n",
    "**解决:** 将词映射到低维稠密向量空间，捕捉词汇的语义关系。\n",
    "\n",
    "### 📚 循序渐进学习\n",
    "\n",
    "**第一步: 理解问题** (你现在在这里)\n",
    "- 为什么需要这个技术?\n",
    "- 它解决什么问题?\n",
    "\n",
    "**第二步: 学习原理** (接下来)\n",
    "- 这个技术如何工作?\n",
    "- 核心思想是什么?\n",
    "\n",
    "**第三步: 实际应用** (最后)\n",
    "- 如何应用到实际项目?\n",
    "- 如何解决实际问题?\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "",
    "## 🔰 新手必看",
    "",
    "**第一次学？这些提示能帮到你！**",
    "",
    "### 💡 学习建议",
    "",
    "1. **不要急** - 慢慢看，不懂的多看几遍",
    "2. **动手做** - 每个代码都运行一遍",
    "3. **改参数** - 试着改改数字，看看会怎样",
    "4. **记笔记** - 把重点记下来",
    "",
    "### ⚠️ 常见问题",
    "",
    "**Q: 代码报错怎么办？**",
    "- 先看错误提示（红色的那行）",
    "- 检查是否有拼写错误",
    "- 确认缩进是否正确（Python对空格很敏感）",
    "- 复制错误信息搜索一下",
    "",
    "**Q: 看不懂怎么办？**",
    "- 跳过难的部分，先学简单的",
    "- 看看前面的课程有没有遗漏",
    "- 多看几遍，理解需要时间",
    "",
    "**Q: 需要什么基础？**",
    "- 会用电脑就行",
    "- Python基础最好有，没有也能学",
    "- 数学不好也没关系，我们用例子讲",
    "",
    "### 📌 学习技巧",
    "",
    "- 🎯 **目标明确**: 知道这节课要学什么",
    "- 📝 **做笔记**: 重点内容记下来",
    "- 💻 **多练习**: 代码要自己敲一遍",
    "- 🤔 **多思考**: 想想为什么这样做",
    "- 🔄 **多复习**: 学完了回头再看看",
    "",
    "---",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 课程5_第2周_特征向量表征\n",
    "\n",
    "**分类:** 其他\n",
    "\n",
    "**🎯 让我们从实际问题出发**\n",
    "\n",
    "---\n",
    "\n",
    "\n## 💭 开始之前:想想这个问题\n\n学习 **课程5_第2周_特征向量表征** 能帮我们解决什么实际问题?\n\n在日常生活中,你可能已经在不知不觉中使用了这个技术:\n- 📱 手机App\n- 🎮 游戏\n- 🛒 网购\n- 📺 视频推荐\n\n让我们一起探索这个技术背后的原理!\n\n### 🎯 为什么需要这个技术?\n\n**问题:** 传统方法有什么局限?\n- 效率低?\n- 准确率低?\n- 不能处理复杂情况?\n\n**解决:** 这个技术如何改进?\n- 提高效率?\n- 提高准确率?\n- 处理复杂情况?\n\n### 📚 循序渐进学习\n\n**第一步: 理解问题** (你现在在这里)\n- 为什么需要这个技术?\n- 它解决什么问题?\n\n**第二步: 学习原理** (接下来)\n- 这个技术如何工作?\n- 核心思想是什么?\n\n**第三步: 实际应用** (最后)\n- 如何应用到实际项目?\n- 如何解决实际问题?\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 本节课你将学会\n",
    "\n",
    "- ✅ 理解核心概念和原理\n",
    "- ✅ 掌握实际代码实现\n",
    "- ✅ 知道如何应用到实际项目\n",
    "- ✅ 理解这个技术解决什么问题\n",
    "\n",
    "## 💡 学习建议\n",
    "\n",
    "1. **先理解\"为什么\"** - 这个技术解决什么实际问题?\n",
    "2. **再学习\"是什么\"** - 这个技术的原理是什么?\n",
    "3. **最后掌握\"怎么做\"** - 如何用代码实现?\n",
    "4. **动手实践** - 运行代码,修改参数,观察结果\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 课程5_第2周_特征向量表征\n",
    "\n",
    "**分类:** 其他\n",
    "\n",
    "**🎯 让我们从实际问题出发**\n",
    "\n",
    "---\n",
    "\n",
    "\n## 💭 开始之前,想想这个问题\n\n学习 **课程5_第2周_特征向量表征** 能帮我们解决什么实际问题?\n\n在日常生活中,你可能已经在不知不觉中使用了这个技术:\n- 📱 手机App\n- 🎮 游戏\n- 🛒 网购\n- 📺 视频推荐\n\n让我们一起探索这个技术背后的原理!\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 本节课你将学会\n",
    "\n",
    "- ✅ 理解核心概念和原理\n",
    "- ✅ 掌握实际代码实现\n",
    "- ✅ 知道如何应用到实际项目\n",
    "- ✅ 理解这个技术解决什么问题\n",
    "\n",
    "## 💡 学习建议\n",
    "\n",
    "1. **先理解\"为什么\"** - 这个技术解决什么实际问题?\n",
    "2. **再学习\"是什么\"** - 这个技术的原理是什么?\n",
    "3. **最后掌握\"怎么做\"** - 如何用代码实现?\n",
    "4. **动手实践** - 运行代码,修改参数,观察结果\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***特征向量表征***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 上节课我们介绍过表征单词的方式是首先建立一个较大的词汇表（例如10000），然后使用one-hot的方式对每个单词进行编码。\n",
    " - 例如，单词Man，Woman，King，Queen，Apple，Orange分别出现在词汇表的第5391，9853，4914，7157，456，6257的位置，则它们分别用O5391，O9853，O4914，O7157，O456，O6257表示。\n",
    "\n",
    "② 这种one-hot表征单词的方法最大的缺点就是每个单词都是独立的、正交的，无法知道不同单词之间的相似程度。\n",
    " - 例如，Apple和Orange都是水果，词性相近，但是单从one-hot编码上来看，内积为零，无法知道二者的相似性。在NLP中，我们更希望能掌握不同单词之间的相似程度。\n",
    " \n",
    "③ 因此，我们可以使用特征表征（Featurized representation）的方法对每个单词进行编码。也就是使用一个特征向量表征单词，特征向量的每个元素都是对该单词某一特征的量化描述，量化范围可以是[-1,1]之间。特征表征的例子如下图所示。"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAADnCAIAAACYFWtPAAAgAElEQVR4nO3dcYwj130f8N8bcvdudXfri+zasqRAXIncOttN3KJFkMyirlSgQLgLu+sk2KRBC7mxb9giSkkUFQoUm8Rt162lRVvSdVwtC1k+pEjihXPaf5Y8I23WTbSU4sKy0y7WwpLFXWLr7tzWlnw+3Z12l/P6x5DDN8MZcnbJ4bw3/H4gQLfD4fD93ps3P86bNxzGOScAAAC5aVEXAAAAoD+kKwAAUADSFQAAKADpCgAAFIB0BQAACkC6AgAABcQ2XV2/fj3qIgAAwNDENl099dRTyFgAALER23QFABCqao6xXDXqUowRpKs+qjnGmMdO2SgtMMbYQqkRSbEG0ygtuEKq5tyxjG1XdAZezbVav7vO1NXaedtiEtUJWL16DOJuHb5a1DxaCZCugilvOXft6nqhput6RKUZVHppRae9g86+W90qE9U2t51LjOVsFKWTR6O0sFjWi/WNGNVDo7TAMoX5Cm+rGOVF9Q9kJ1LdKhuG0dWpY6aaY2yROg1dL1Iho3aORroKwCgW9fKa2KGrW2W9uLoSXZEGlF5a0YXs1DjYI13vWjKXiap8UqjmMgUq1nfzaSKidH6Xc/XzVnW9UDMqYiDZjXpRrxXWVT6MnUijtFY2ljeeLepxzlfVnPubVjq/WzHIeSBTDNJVELPOoztVt8r6ylLXwVwcY7G+xDRKCyxXtc/I5fkOm56d75xNNbY3a8bq5RW9tl8ne4m+spRu/dUVF7XHzOzQctXOaq4wPd8ubc20NEoLi2Wj0spVRI5BQv/Cu4bZpIvL86Q5vbSit081rNDs1XOuJvMaP1SsfRvbmzVjOUvppRUxXzVKCwulRneB/Za7t+pTOVGpbpXJWO3svZbsskG1ze1GNccWSlXX9YwTtqP3rh5yPfCYSqVS165dG3w7FYPIqPB6USe9WOecc/vf4jJrqVFxvovXizpRe6WKQcL6UWsXsRNPZwmvGI5o7cAqBrX/qBjk+rfwR59q4TLXTCuYrgIFKLzwT+fOIQu/Ujl3Bru9XMvtF4TN9FwuY/sKATpqw1FgISq/5Zy7urlHJUTHVVDHcr1YFztv54WT9FPPXT30esDZVTDC6Jl1LuL+3kLp/G5niCUz17k2ZH9Fzy4bZJ+/RC673B68r+9bZ1KdJdbZY5qoPXjUDiy7URGH/NsvZJcN5x+dMH2rhaStGbKGf/uMj3kUvnGwR+16Sy8JJ6vqa2xv1vTis62GTOdXDas3+C0nIjnbt31uRdQ1JC4UOJ1fde7n3suFbfpWgrQ6vZroxP3Uc1cPvx6QrgJK51eNWmG96tjfnYTz4EyhNvISnlQ7O3VyU3bZoL2DBlW3yjQ/687Hp6VYtVhm85eLennxZGNY4ghrY3uzJt/Fv/TsvFfeaBzsBXhzrZCxx3kWy53t+C2XUWN7s0blRWF/HNIRVbJKSM/Oey6v79f8uvaJ+qnvrh5uPSBdBWYd3nPrBc9s1SgtCPOtrFNoyVnfoaoHe/YOnJnTa5vb1YO9oU0KVLBaWtL53YpRK2ROOgDf6q+Zwrx44UsWmTnd++wgQIu7R3baX8b9lkuoNdGko17UPfOV31Qj3ylIslVCZ6hEVN0qexf/VP3Ua1cPtx6QroLLLhtULgeZ311dV+E0Ir20otcKi0L67V5iBb3YPmZb042ePeUuqEa1dFhDn4uBE1Z1qyz01qgPWJ7S3WeN1VxGGO9Nz87bh7lqbrHcft/SiufoqN9yOXVPNHGOB9oV09jeFM9C/JaLG5GtErLPFvXyojjdoZpji2W9eLnvl6gA/dRzVw+/HpCuTiC7bBD5HK1bxwHrLHhtrmiMvHQnl15a0YnEr1utJWKXzm5Yd+ZYp/d79sTugB+hYLUIshv1E4wJZjfqK5udsRA5poi5pfO7vDIvDtns6Tp1DmzZDbu5t5Y7X7PT+d16cW+xKzi/5TLymhbpOMQaldX91gkDiXPA/ZbbG5GxEtL5XaHntjov9+m9J+2n3rt6+PVw8tkZahjWzECAgNyzsYT5VfKzEpMyxR0+55TI/svHWlS7Os6uAIZHOFMNNn9BFun8Lpd0ABOkFMWujnQFMBzp/OUidUbZMoX5Co7/EEdR7eqMcx7+p0RgZmZmZ2cnlUpFXRAAABgCnF0BAIACkK4AAEABSFcAAKAApCsAAFAA0lWXam5UjzuQ6YG94xk1jWvg4xm1aPg1MD6R9vikkGsg/Fu7onHa24S7fkhf4HwsRvdS5zu6fjxfePqG14dFyVUQvyjE+nA+RuEktSRN1NyrLFY0Ytgna1P/7cgTuOshGm5dwTsK7d3c/tuRJmqRZ6HcTea7Y3PvfVuNSD17pbjQ+0AgRVsjXTn5Pe7HvZLdzuKjfMR/ezwZx70vdH1cZLpytFcUXQ++EsM+QS1JEzX36cu6Ljyo5+Rt6r0d74+LRo9iOB5S5B1goF8wcD8fToKoRV5F6m4y30h7PU1K7ki9S14xHE+pcu4BMrU10pWD65FiQVqrs45rbefTDtsPRXNtToof6uk6RnlG4XzBFV3wWgr6jlFwP0HO+rsiLD1Nm3ptp/v90enx3DyxfH4BqtvcNo8a8Goyv3L7xyN7pCdtO9naGteuHHo8DsZbo7Tm9xPtwjPOshvc57cl24+YilSvqDtRZOZ0Kq+1fpnaP2wv7tWliJq6Aq+uF6h4Od/jIVWB2rTHdqQI3Le5nc3UI8A+ZG1uW3cN9G96YV2PX8ptkTzSHiU/rdG2NdKVyOtpNvZPGouXK+1HmYk/PyIez8naUwKJ+lFuzqh9o0jnd+vtX17JbK7UHb+6EryW2qKOmtyBN0pr5a6nRJ+iTT23I4g6cN9HNjkfBtuTZ3P33E7UUYu6aqBHk3VH2jjYI5227GcZuutA3kj7lZzI64uoTG2NdCVyH42yG/ZpaL1IhYzdYNYPgnLOeYUWhWcpVIzO0zTX9oI848z1nOkoOKPuEUVje7NmPRiexOcWnqyWiEiKqMkZeKP0dIG6j9UnblOf7bRJELhfyq2uF2q90mybb3P7b0eCqEWuGvBtMv9Ia3tzl9u7tlErPG2/IHek1KPkRGRVRefZZ/K1NdJVQOn8quH1oGzrEX7tx3YKzctX5084sCgNzyjs541u5PO7nFcMr8dABaslCbUGgzzHa0/Spj22I7cTDu9aPJr7VNuJVKAmc0fa2Qmyy4ZUJ1T99Ch5NefzPC8iSdoa6UqUmevz5fkE6SfgOPGJr5YNX6+oO1HU92vuMWq/Xto/HgmiJjHw6laZ7HOoTKFm/dF1C0nfNu27HQkC92zu6noh6EBgFzEe7+1IELXIUQMBm56IWpGmZ+fJ9/xB5kh7l7yaY4tlo9LnamXUbX2K6RlKON3MQMe8FmF6p+P+E3GGtt+cVufMba+t93z7aPnO5vGfxd2Z73qaWpIiau4XuN/MuaBt2mM7UgTuM0E1wGRB7t/cvbYjRdSinjPUhUmhPpH26RYSR+pXcu/bB+Vra6QrB+cRxnE7nM+No373y/reXut8VYqZr/Wu6bueUTjD8LzDMEgtcUmi5n6Jyfveo+Bt2mPrUgTeVS7fm3G8AvRtbv9DlRRRi3yn8jte6Bepx54tf6ReJfe47VfStka6chrxjX6S3Fc4nlHzcQ18PKMWhVSk8Yk0oo/DtSun9NKKPropAY3tzZoMl6XHM2oa18DHM2pRODUwPpH6Cb0GQkuEETvtbwZyXjH8hvGHTaahg/GMmo9r4OMZtWj4NTA+kfb4pHBrgHHOw8uFEZqZmdnZ2UmlUlEXBAAAhgCDgQAAoACkKwAAUEH3+OC1a9eiLhQAAAC99NJLuHYFAAAqwWAgAAAoAOkKAAAUgHQFAAAKQLoCAAAFIF0BAIACkK4AAEABSFcAAKAApCsAAFAA0hUAACgg6nRVzTGWG9XTWAAAQFUnSFfVHBMslBrhlUpV1Rwbq+wb63gbpQV7b/eLUegTQodw9hRVuspp4xXfp1jIFCxqv3WCvFdOg0RNFGHHD/ZYrIpB5HjG17AechzaA71O/3jGU6oYrRqV8QltIYh5vBWjs8OL//ZZpV7UOzUh63P6ehkkXod6UR/V0wAHFyBq33WCvFdOg0QdbccPlK5CbIy4pKtWFal4oDqVmMfrOuZ6HoJdsQt/qlcrg8XbYy2pBYnab50g75XTIFFH3fGDDAZWt8pkrObTvit4njY2SgssV7VHD4Thgc7qC6WDvtup5thCqWq9Iu8YQ3aD890eVRQ3MY+3vl+j+Vk7vPTsPNX26z3fkpnTae9A1v2zj2HF2yitlclYzoZSyKELErXfOqeoMUkMEnXUHT9Aumoc7JE+l/F9ubSQKcxX2ml4b1HIKeXFtblWTq4VnraWN0pPF2qtxLy6XygH2E6tsLi/ynmsD5AgkcbBnnOB17E5M6dTec3eSev7NfHV8qJCl3GGEK+1ne3Nml58VpFsFSRqv3UC1ZiUBok67LL1FSBdOfbLrmutzj00nV81apvb7biMSivBZJeNVnp2rJ/dsAdC+2xnQ5EuAGMjnd+tGLVCptUd1vb09ivZDXv0ol6kQkaFjNWff7xt1fVCrecwDMBAAqSrzJzeOVts9cR6UdxXO/swY4vl3ifFrtNMh5NsJ2rqzgqCbq7WTM/OO1/32WuFvMRX571WSedXDRK+eMlpKPGqNRBIwaL2WydojclnkKjDLltfAdJVemnFMQrQzX2lrtepUK/TypNsJ2rp/K4S5YQg3K3p2kv7jIcTtS7w+h2rZejpPQ0h3up6QaGBQCIKFrXfOievMVkMEnXkAk3IsE6mvKeK+E1pdU5178wkEaYZts7RrOV+2znlHJSRT2TnnKs1L2oYYhuv38T0elH3miXrmvbr3O9VqKEB4vVbooAgUfdcR8lbFwaJui2igAPed8Xd43+Owrpe6qQfr3Qlrm5UHGt5bkeNdGVfg2tTruuezBjEK4TYCU7s0mIduO9JVLBeTh0v9/+uKb++Ufut02O5/E4fdbQdn3HOKY5mZmZ2dnZSqVTUBQEAgCGI+jcDAQAAAkC6AgAABSBdAQCAApCuAABAAUhXAACgAKQrAABQANIVAAAoAOkKAAAUgHQFAAAKQLoCAAAFIF0BAIACkK4AAEABSFcAAKAApCsAAFAA0hUAACgA6QoAABSAdAUAAApAugIAAAUgXQEAgAKQrgAAQAFIVwAAoACkKwAAUADSFQAAKADpCgAAFIB0BQAACpA0XVVzjOWqUZcCAABkccp01SgtMMFCqTHcYklGDNcvi/Zap5rr8UZpDRB1kLdKKkjRqznPXV98q3pd4vSBC0vVDH3sOvgg8Ubat/mp1Is66cV654/OX0NRMYiMyiBbSKVS165dG1ph2uGJ/w62TsVo1fRg8YzcAFFXDDvYEPaNUAWIWlxcL+p2w9aLur22X4XJa4DAh9BbozVuHXyQeIO8NzzDSFch7K8SpStXqK4/+63TalHlOvRgUYtUCj1IRK54fMJTKWo+aOCKBesybh18kHiDdfnwhHHtyuNssZpzjBE0SgvtP6UfNqrv12h+Nt3+Mz07T7X9etB1shuc7+bTpJrBolbVKSLKzOm0d+Aa/WqU1spkLGdDKWQYhhW4isatgw8Sb9RdfhjpytE5G6WFTGHeHiUoL7JclSi7bFBtc7u1cze2N2v6ylKaGqWFzP4q76wsX8JqHOw5F3h00yDrqGVoUSt14A4UUWZOp/Ka/dWrvl/rvL/91StTmK/wDTWCJho8cCIqLyp65WrcOvgg8UZeDwOkq1oh0+qc+6u83Tmr64WaUWn31OxGxaDyVpUo+2xRb+crO1tROr/b6dUK7wLgrVF6WtwdYiGd360Y9r7P1vZ08aX2Vy9alHSw4PT8A89u2KM19SIVMqplLFDFAOmqNWhZL+rlxf47aHpppZWvOtmKHGOBmUKtzzaikJ6ddy5wnQ8HXUctQ4m6mssUqFhXJ1kFbUfh8MxX571W6XxPU8LwAk/nV8VxFEmJVyDWfzheHXyQrh15PQw+GJjOXy7qtcJ6v66Zzq8atcJ6tbG9WTNW82lyDRxak43k4zrpaxzskT6XOfk6ahk46mqOLZaNilrD+idvx+qWOmOdPQw5cNmP5J3TYM43fmXMOvggXTvyejjdDA3/mYEVg5wzXF2rkfBy12xYxxRJSWYG+s7ZFedo95vXq9DEoZaBonbsBCoJErXANbW7s4JjOyoYIHDhtgU1W37cOvgg8UZ8A8Nw0pUzL9k3InTt6NYrQoTCGZVeLBpSpivuE5GrJ3tHLSz1rBGZnTZqj9PkOEUtNqkjLkdbq3Ls6jh14I72VqilBePWwU8fb6/De/gY59x9bAlPo7SQ2Vypj2KEaGZmZmdnJ5VKhf5JAAAQvpH+ZqBjkgUAAEBgo0xXyFYAAHBKyRF+Vjq/y/Mj/DwAAIgNSR8gAgAAIEK6AgAABSBdAQCAApCuAABAAUhXAACgAKQrAABQANIVAAAoAOkKAAAUgHQFAAAKQLoCAAAFIF0BAIACkK4AAEABSFcAAKAApCsAAFAA0hUAACgA6QoAABSAdAUAAApAugIAAAUgXQEAgAKQrgAAQAFIVwAAoACkKwAAUADSFQAAKADpCgAAFDDsdFXNMcZYrjrkzQIAwHgbcrqqbpUNw6DyFvIVAAAM0VDTVaO0VjaWN54t6jHLV43SAmvzO3P0WUdcvFBqjKjAQzGkqBU+1e4Xhvi6oo3cEazNvPdna1BFxfjHs2vbVOvjfHjqRZ2MivgP4QWRXqx3vyC+YQhSqdS1a9eGsqmK0Smy+O8A69SLur2231slNUDUFcNuzXpRVypqUZAacKgXdVVjDRZsvah7dFRxde81pDWeXdumXB8fYroSkpSj3zqb2JWrxJCHGvHQ0pWrZJ4FDbKOVRGq9OTxjFoULDqRqpHywZrStVShWhjznVzB8Ic3GNjY3qwZy1kiIkovrei1ze0GEVHjYI/0laW0vXy/bq+vF5+13kDp/KrRfodc6vs1mp9Nt/9Mz85TO4QTrdMorZWpXUHSG1bU6jppdGq1r0ugYKtbQQLMzOm0dyBhR+42nl3bpmAfH1q6amxv1qi82BrLzBRq1Mo+6dl5slPX9mZNn8vYb6oVMvbw52JZyoNd42DPucCjO/Zaxx7hzRTmK3xDkV160Kg7K6nZkwNGJ67v+PalmGBNebBHOm11Xa/JzOlUXrOv3dT3a6Mo8hCMZ9e2qdjHh5WuquuFmuN0sF7UhbOlVl7KFOYru3k7V3edVyrX5P2l87vtM2ZaVHrewSk0Sk8XakYlhs3qVl0v1IxVYdeOp9re3GV7AKhWeLrUIErndytG54vn2p7efzuxMNZd2zbSPj6kdNU9TmCPB1a3ykJW6kSVXlrRa4V12ds4PTvvXOA6PQ66DlF2o6LMFP+hRF3NZQpUrKuZrIK1aYuy55AtwYPtLMwuG/ZwSHaj85Vzdd6/nuQybl3bNZVPxT4+nHTlNardTkfZjfrKZmfMr/MtJJ3frRf3Fj1ekYrr9LdxsEfieGbgddQycNTVHFssG45zabWcoE2r6wV1BwKJKFiw6dl56n9VKtgFLjmMWdfunA1apw0q9vHhzdrw5p7ZOqpJJMObyO6IQCi+OH/TZx1xsqtic3wHiNr6tzqh+ghSA/ZrKk5jFgUKtv+MdcWqYjy7tk29Pj6SdOWaCalYuuKtprG49tHOn57riEuV26FPG7X7NjtS6hAmClIDih6qugUO1r03i/u4cg09nl3bplgfZ5xz9+cOWaO0kCl0ZgsZo5lEMzMzs7Ozk0qlwv8oAAAIXTL8j0jnd3k+/I8BAIAYwwNEAABAAUhXAACgAKQrAABQANIVAAAoAOkKAAAUgHQFAAAKQLoCAAAFIF0BAIACkK4AAEABSFcAAKAApCsAAFAA0hUAACgA6QoAABSAdAUAAApAugIAAAUgXQEAgAKQrgAAQAFIVwAAoACkKwAAUADSFQAAKADpCgAAFIB0BQAACpAvXVVzjOWqUZcCAACkMlC6quaYQ2yzTKO00DdIv3WESlooNUZT3uEYJGqiVuTK7RMDRB3krfJCc/sV3rsLu45+SnXu00Ytvi+SwPkAKgaRURlkC+FtNJVKXbt2bQjlsUqkF+td/w6wjri4XtRDqK7QDBB16y8iUilgzvmgbd0Otl7Uvd8qLzS3T9S+XTiUo99IDBK1Q72oj3Y3R7rqx9Ukni3kt44rFoV28EGitnd2heK1DBa1SLHQ0dzdf1r8u7By4bYMFnWPtUYgjGtXPc40+46iLJQOQijRIOr7NZqfTbf/TM/OU22/fvJ1iCgzp9PegRJjBoNFnd3gfDefJtUMsa3VguamYE2pUBf2M6yoG6W1MhnL2VAK6WPo6apRWsgU5u3z5fJiJy35vdQoPV2otbL06n6hPOwiDaRxsOdc4NF0vutk5nQqr9lju/X9WoglHaaBolbW0KKOoicPAs1NRN4R9e7C5UXlrlwNIWprO9ubNb347Gj38YHTld1g1ulSdb1QMyobrSCyGxWDyltWvvJ7yRF3dsMeBo+BdH63YtQKmVYFre3pUZcIwtf6+rWhTLYCf/5dOLthj1HVi1TIqJOx+ut/4KquF2rG6qhPqQdOV8LY5Sn7p+vkVDLp2XnnAo/i9lpH2Kn56rzMkYoGjVpNQ4m6mssUqFhXKlmhuYnIN6L+XTidXzWotrmtQr4aStRRDR9IcN+V5IMKruI1DvZIn8ucfB2i6pY6I0TDi1olA0ddzbHFslFR7kIOmpsCRdSzCyuSu4cQdXW9MPqBQCIa/kT2ikHOuZ6u2a4eLwn/rBd1IrlmBvrOXhVnKweY4eo3O1hSw4havblTA0Xt2L8Vg+b2i1rgmgDeiVWtlh8gar8loxLGRHbh6pM7KJ+XWlmKiIxKvajLla64T7Fdreu5jnghTqFcZTl11I6wVYv9tFF39mEVo+Zobp+o/bqwo70VCphzPkDU3JXtRoxxzt29LBZmZmZ2dnZSqVTUBQEAgCGQ4NoVAABAP0hXAACgAKQrAABQANIVAAAoAOkKAAAUgHQFAAAKQLoCAAAFIF0BAIACkK4AAEABSFcAAKAApCsAAFAA0hUAACgA6QoAABSAdAUAAApAugIAAAUgXQEAgAKQrgAAQAFIVwAAoACkKwAAUADSFQAAKADpCgAAFIB0BQAACkC6AgAABSBdAQCAApCuAABAAUhXAACggFGkq1u3bn32s5+9fv36CD4rKq+99tprr70WdSmi8cYbb1y9ejXqUozO1atX33jjjahLMTrXr1/f2tqKuhTRuHXr1tbW1v3796MuyEh961vf+trXvhZ1KbzwkJmcP/LII8lkcnp6+u7de2bYn9eWSqWuXbs2ms/64ksvnTt37ty5c1euvDyyACXxRzs7DzzwwPnz5z/zmX8zDrE/88wz09PTFy5cqNVeHYd497/97fPnz58/f/4f/uqvmqZpjkPMbT94663p6enp6emPfOQj4xN7pVq1ov7c5z4nW8Thnl1xonv37n3/+98/Pj42TfPGjZucEw/1I0eOc76zs/POO++88847V15+2YxdgD1woldfffXw8PDOnTubm5ucE+dxjp5z/ievvHL79u179+7tfG3HNGMdLREn+uY3v0mM3blz56tXr5rxjtaJE337229wTrdv3/76179+9967RBTz9ibinL/yyit37969ffv2lStXTNl6dKjJ0DTNo6b54he/9JM/9eHf+ldrdw+bh8cj+pIymrMr0+RNk//Pvf2HH3nksdTM/sH/fveo2TT5mHwRa5rmX3znzQ/9xNx73/u+2p9+4/5R87gZ2y+hJufHTfMP/+sfvfe97/vJn/rwje/9v8NjsxnXaDnnnJsmv33n7s/8rH5hevrLX3n5/mHz2ORxDlhgmvzo2Pz4z//C1NTUfyj9x/tH5nG8G7t9NLN69Ac/+PCrX/+GbEczxsNMnpzTscnfPTYPj03G2ESCTSa0ZII0xsL7UMvMzMzOzk4qlQr1UziRafKjJn/3mDdNM6GxyaR2JqkxRqFHGDXOySQ6OjYPm/y4aWqMnUmyyaSmjaB1o8CJmk1+ZPLDY7NpUlJjk0k2kWAaY7EMmHMyOT82+eExP2qaCY1NJNhkUkswimfATian46b57jE/bnLGaDLJJhJaUotz7MLRzGyaPKGxM0ltIsHk6dGhT7VgjDTGEhpjRIwxxihmR3Jr92XCfzKdPIeMc/tYzcbgKMYYI05EjBFnjOIdbqtBufXvVvclTjEPu4O3w6Z2F4+/9tGMMWodqKXq1MlwN8+IcUpqZOXFBCONERGPTdszIk48obGJJGmmltBYQiNGPGYp2RNjpBFLMJ7UmKYlGPGEZh/Roi5cGDiR1dZEpqYlGCVYbE+tLIxI09gEkcaIMZaIdbAujEhjlExY37ApqZGmxbxbex3NGMl0NAv37Mpq8lf+5I9/cfmjtVf+ezJpnVbKEvxQMMYSGvvif9740ovlpMYSmjZWfVrT2JU/+PILny81jw41RjE+52CMNEZHh/f/0+dLW1/ZnEhomhazfbkLo4RG/+0Pv1r8d8/fuf3DhEZxP2KLWIKx1//Hnz73mX/9/f/7fxIJTSMe+9ito9mVr3z5Sy+WJxIsIdvXsbAvjt29d+/ChQtEND09/d0bN+I3kd00+XPPPz81NTU1NfXcc8/H/XKs25UrV86fPz85OfmpS5eacZ/sa5r80qVLk5OT58+f/4MrV5rNWEfLOef8G994fXp6OpFIPPnkU1JddR+BN2/cfM973kNEjz/+eHNs5phYPfrs2bO/9swzsvXo0CeyT06e0TSNiEzTnJw8G8u5vzdu3Dg6Ojo8PPzujRuxn8wt4pzfvHXr+Pj48PCwXq+bZtQFCl+93jg8PDw6Pr5x8xanmLc15/zW924RUbPZvHnrptV3Yx1xByd66+23rZDfevvtd+7e4zzerU1ExDmv1xvNZjC1x3sAAAXpSURBVPP+/fs3b960GjzqQnUkPv3pT4e4ec6bnJY++ndv3/7R8/++9PgT6ZEN95dKpU984hMXL14M9VM4kcn5X/8bP31wUP/pn/nZz/zb5ycnktaEEqnOocPAOXFif/kn/sqbN248lpp58Uu/MzV11oo8lqFbba3/zY8c1OuLSx/9p//sn2tazK9dcaLUTPqtt9++cGH6i5f/y8WLFzVGTJp5YqHinD/44PumHjh37/79Fy//zqOPPqoxjTEe4/bmnExOH/6rf+27b775WGrm81/YmJqasibHSRJ06BPZj5rm4TE/MjnnfDKpTSZYMjGKHX6UE9kPj/m7x2aTU1KjM0ltckwmshNxTofH5v0j0+SkMTozoU3KNO11uDjnTU6Hx/xQaOvYT2S3pjUfm1xj7EyCTU6M0UR26yaNw2OTMTqT1M4ktUSsL0zbPdpq8eQYTmTXGNMYaYwSmvWPuB3Jrdn5Wus/xhjj8fvpDk+cOOeMkdWHtVi1qhfGrKlDWmdPjjnGmNb5j1p3ocQ/7hbra3WiNX/KmlYT647d6tGdRieS676F8CeyM0ommKYxImp9D5Uo/EFZUz+TGqOkZnLezs1jcCRrT2TXGE0kKKGRxpg96zeeuDVviiY4TzKmaSwR31Mrsg5TnDSNJhIs0erImrUwvm3cwYgnGJtIMs1kjFGifRdS1OUKUbtHc6vFNY0lNSbV7NfwJ7Jr7NO/+RtPpH78X/7WbyQ0ic4rh4Uxdvv223//7/3SP/iVX77zox8mpGresDE6Orz/67/2j37x4x978zt/rsV6mMiao/+dv/jzX/j4x/7Jr//j46N3E/EN1pZg7DdX/8XSz/2d/b3/ZZ1Gj0HQFqYxeuELv/23/9ZC7ZU/TiZi+3MtDnaP/vmP3fnRD6W7VSPsqYc/+MEPzp07R0TT09N7e/sjmxc5uonsnF+6dCmZTCaTyXyh0GyO0WRf0+TPPbc+NTVFRE8+9VSzGfPJvibnTz75FBGdPXv2t7/wBTPuE7tNk1+58vKF6Wki+tCHPhT79nV5/fXXrYnsDz744J27d8chdrFHZxcXZWvx0CeyT1/8sYsXL05NPfDAA+d+PJUyicVt9JfTE+nMxMTExMTEQw990Iz36HaXJzJPJBKJicnJJ55ImxTni3bWLQrpdHpicjKZTP6l9z9kch7zixnEH3vsMSJKJBIfeOih9pEr6kKNBOf8/R94yPr3ey5enJw8MyaXpK0effbs2VTqcdl6dOgzA5ucf+fNm1svby199GOPPvLwZHJEP+UyupmBnI6OzRdeeIETfeqSMZnQkrG+pGFr/QRqk//u7//e925975OXchfOTU0kY/tTNdYs0B+9c++FF154//s/sPJLvzyRZBMJjcX0ioZ1dD5qmpXK1T/7s299ysi978Efk2qeWKisfXv31de+evXqJy8Zjz788ESCJeL9E7ftHzX+3d/7/bffevuThjGZ0KTq0eGmK6vJrcmgRBTbiexN/u6R2eSUYHR2Yhwnsluxj8dEdvPdY845JcZrIjtpzLpJg2Eie1yJv8h+3OQJjcnWo8OdGWjt2BqjpKZRa2Zg7I7kjLV+3ZVz62sI5zx2vzvvxZr2SpTQiPH2xG5ZduwQMKuJWYJxzigxBneDW/dlJBgjjWt21411yALOrNlxCY0Rbx+xYz0tsvXr+9YzNEjTpGvu0Ceya4wmEsy6Kce69WpkzX327NmwP4IRadT62WbOrQmvsf267dL6RXaNJpnGOVk3YMk2k2iYODFG1k0LRK0uzSi2P3PAGBHnSY2xJJmcEbVusIv3Edtm3XFFjBKMiNpjQrEOvNOjk2SapGlaQiOpenS4g4HUGjJqfcgof6Hn1q1bDz300Ag+yPpVLfunELW4n2OIrKAdsce6R7famlqRa9I9D2jIrJ+Ms595H/v2deGcc+sIyawnnMW4qVsk79GhpyuLna7iirf/F+MY/fD21+0xCX3c2nrc2lc0nrHbKUG2qEeUrgAAAAYR+m8GAgAADO7/AytSyWSTAYajAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 特征向量的长度依情况而定，特征元素越多则对单词表征得越全面。这里的特征向量长度设定为300。\n",
    "\n",
    "② 使用特征表征之后，词汇表中的每个单词都可以使用对应的300 x 1的向量来表示，该向量的每个元素表示该单词对应的某个特征值。\n",
    "\n",
    "③ 每个单词用e+词汇表索引的方式标记，例如e5391, e9853, e4914, e7157, e456, e6257。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 这种特征表征的优点是根据特征向量能清晰知道不同单词之间的相似程度，例如Apple和Orange之间的相似度较高，很可能属于同一类别。\n",
    "\n",
    "② 这种单词“类别”化的方式，大大提高了有限词汇量的泛化能力。\n",
    "\n",
    "③ 这种特征化单词的操作被称为Word Embeddings，即单词嵌入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 值得一提的是，这里特征向量的每个特征元素含义是具体的，对应到实际特征，例如性别、年龄等。\n",
    "\n",
    "② 而在实际应用中，特征向量很多特征元素并不一定对应到有物理意义的特征，是比较抽象的。\n",
    "\n",
    "③ 但是，这并不影响对每个单词的有效表征，同样能比较不同单词之间的相似性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 每个单词都由高维特征向量表征，为了可视化不同单词之间的相似性，可以使用降维操作，例如t-SNE算法，将300D降到2D平面上。如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "② 从上图可以看出相似的单词分布距离较近，从而也证明了Word Embeddings能有效表征单词的关键特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 之前我们介绍过Named entity识别的例子，每个单词采用的是one-hot编码。\n",
    "\n",
    "② 下图中，因为“orange farmer”是份职业，很明显“Sally Johnson”是一个人名。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "③ 如果采用featurized representation对每个单词进行编码，再构建该RNN模型。\n",
    "\n",
    "④ 一个新的句子：Robert Lin is an apple farmer。\n",
    "\n",
    "⑤ 由于这两个句子中，“apple”与“orange”特征向量很接近，很容易能判断出“Robert Lin”也是一个人名。这就是featurized representation的优点之一。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 可以看出，featurized representation的优点是可以减少训练样本的数目，前提是对海量单词建立特征向量表述（word embedding）。\n",
    "\n",
    "② 这样，即使训练样本不够多，测试时遇到陌生单词，例如“durian cultivator”，根据之前海量词汇特征向量就判断出“durian”也是一种水果，与“apple”类似，而“cultivator”与“farmer”也很相似。从而得到与“durian cultivator”对应的应该也是一个人名。\n",
    "\n",
    "③ 这种做法将单词用不同的特征来表示，即使是训练样本中没有的单词，也可以根据word embedding的结果得到与其词性相近的单词，从而得到与该单词相近的结果，有效减少了训练样本的数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① featurized representation的特性使得很多NLP任务能方便地进行迁移学习，方法是：\n",
    "\n",
    " - 从海量词汇库中学习word embeddings，即所有单词的特征向量。或者从网上下载预训练好的word embeddings。\n",
    "\n",
    " - 使用较少的训练样本，将word embeddings迁移到新的任务中。\n",
    "\n",
    " - （可选）：继续使用新数据微调word embeddings。\n",
    "\n",
    "② 建议仅当训练样本足够大的时候，再进行上述第三步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 有趣的是，word embeddings与人脸识别与神经风格迁移中介绍的人脸特征编码有很多相似性。\n",
    "\n",
    "② 人脸图片经过Siamese网络，得到其特征向量f(x)，这点跟word embedding是类似的。\n",
    "\n",
    "③ 二者不同的是Siamese网络输入的人脸图片可以是数据库之外的；而word embedding一般都是已建立的词汇库中的单词，非词汇库单词统一用< UNK >表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① Word embeddings可以帮助我们找到不同单词之间的相似类别关系，如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "② 上例中，特征维度是4维的，分别是[Gender, Royal, Age, Food]。\n",
    "\n",
    "③ 常识地，“Man”与“Woman”的关系类比于“King”与“Queen”的关系。而利用Word embeddings可以找到这样的对应类比关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 我们将“Man”的embedding vector与“Woman”的embedding vector相减："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "② 类似地，我们将“King”的embedding vector与“Queen”的embedding vector相减："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "③ 相减结果表明，“Man”与“Woman”的主要区别是性别，“King”与“Queen”也是一样。\n",
    "\n",
    "④ 一般地，A类比于B相当于C类比于“？”，这类问题可以使用embedding vector进行运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⑤ 还可以计算Euclidian distance来比较相似性，即||u−v||^2。距离越大，相似性越小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 假设某个词汇库包含了10000个单词，每个单词包含的特征维度为300，那么表征所有单词的embedding matrix维度为300 x 10000，用E来表示。\n",
    "\n",
    "② 某单词w的one-hot向量表示为Ow，维度为10000 x 1，则该单词的embedding vector表达式为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "③ 因此，只要知道了embedding matrix E，就能计算出所有单词的embedding vector ew。后面我们将重点介绍如何求出E。\n",
    "\n",
    "④ 值得一提的是，上述这种矩阵乘积运算$E ⋅ O_{w}$效率并不高，矩阵维度很大，且$O_{w}$大部分元素为零。通常做法是直接从E中选取第w列作为$e_{w}$即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① embedding matrix E可以通过构建自然语言模型，运用梯度下降算法得到。\n",
    "\n",
    "② 举个简单的例子，输入样本是下面这句话：\n",
    " \n",
    " - I want a glass of orange (juice).\n",
    " \n",
    "③ 通过这句话的前6个单词，预测最后的单词“juice”。\n",
    "\n",
    "④ E未知待求，每个单词可用embedding vector ew表示。\n",
    "\n",
    "⑤ 构建的神经网络模型结构如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⑥ 这种算法的效果还不错，能够保证具有相似属性单词的embedding vector相近。\n",
    "\n",
    "⑦ 为了让神经网络输入层数目固定，可以选择只取预测单词的前4个单词作为输入，例如该句中只选择“a glass of orange”四个单词作为输入。当然，这里的4是超参数，可调。\n",
    "\n",
    "⑧ 一般地，我们把输入叫做context，输出叫做target。对应到上面这句话里：\n",
    "\n",
    " - context: a glass of orange\n",
    "\n",
    " - target: juice\n",
    "\n",
    " - 关于context的选择有多种方法：\n",
    "\n",
    " - target前n个单词或后n个单词，n可调\n",
    "\n",
    " - target前1个单词\n",
    "\n",
    " - target附近某1个单词（Skip-Gram）\n",
    " \n",
    "⑨ 事实证明，不同的context选择方法都能计算出较准确的embedding matrix E。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 上一小节我们介绍了context和target的选择方法，比较流行的是采用Skip-Gram模型。\n",
    "\n",
    "② Skip-Gram模型是Word2Vec的一种，Word2Vec的另外一种模型是CBOW（Continuous Bag of Words）。关于CBOW此处不再赘述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 以下面这句话为例：\n",
    "\n",
    "- I want a glass of orange juice to go along with my cereal.\n",
    "\n",
    "② Skip-Gram模型的做法是：首先随机选择一个单词作为context，例如“orange”；然后使用一个宽度为5或10（自定义）的滑动窗，在context附近选择一个单词作为target，可以是“juice”、“glass”、“my”等等。\n",
    "\n",
    "③ 最终得到了多个context—target对作为监督式学习样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 训练的过程是构建自然语言模型，经过softmax单元的输出为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "② 相应的loss function为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "③ 然后，运用梯度下降算法，迭代优化，最终得到embedding matrix E。\n",
    "\n",
    "④ 然而，这种算法计算量大，影响运算速度。主要因为softmax输出单元为10000个，y^计算公式中包含了大量的求和运算。\n",
    "\n",
    "⑤ 解决的办法之一是使用hierarchical softmax classifier，即树形分类器。其结构如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 这种树形分类器是一种二分类。与之前的softmax分类器不同，它在每个数节点上对目标单词进行区间判断，最终定位到目标单词。\n",
    "\n",
    "② 这好比是猜数字游戏，数字范围0～100。我们可以先猜50，如果分类器给出目标数字比50大，则继续猜75，以此类推，每次从数据区间中部开始。\n",
    "\n",
    "③ 这种树形分类器最多需要log N步就能找到目标单词，N为单词总数。\n",
    "\n",
    "④ 实际应用中，对树形分类器做了一些改进。改进后的树形分类器是非对称的，通常选择把比较常用的单词放在树的顶层，而把不常用的单词放在树的底层。这样更能提高搜索速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 最后提一点，关于context的采样，需要注意的是如果使用均匀采样，那么一些常用的介词、冠词，例如the, of, a, and, to等出现的概率更大一些。\n",
    "\n",
    "② 但是，这些单词的embedding vectors通常不是我们最关心的，我们更关心例如orange, apple， juice等这些名词等。\n",
    "\n",
    "③ 所以，实际应用中，一般不选择随机均匀采样的方式来选择context，而是使用其它算法来处理这类问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① Negative sampling是另外一种有效的求解embedding matrix E的方法。\n",
    "\n",
    "② 它的做法是判断选取的context word和target word是否构成一组正确的context-target对，一般包含一个正样本和k个负样本。\n",
    "\n",
    "③ 例如，“orange”为context word，“juice”为target word，很明显“orange juice”是一组context-target对，为正样本，相应的target label为1。若“orange”为context word不变，target word随机选择“king”、“book”、“the”或者“of”等，这些都不是正确的context-target对，为负样本，相应的target label为0。\n",
    "\n",
    "④ 一般地，固定某个context word对应的负样本个数k一般遵循：\n",
    "\n",
    " - 若训练样本较小，k一般选择5～20；\n",
    "\n",
    " - 若训练样本较大，k一般选择2～5即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① Negative sampling的数学模型为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "② 其中，σ表示sigmoid激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 很明显，negative sampling某个固定的正样本对应k个负样本，即模型总共包含了k+1个binary classification。\n",
    "\n",
    "② 对比之前介绍的10000个输出单元的softmax分类，negative sampling转化为k+1个二分类问题，计算量要小很多，大大提高了模型运算速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 最后提一点，关于如何选择负样本对应的target单词，可以使用随机选择的方法。\n",
    "\n",
    "② 但有资料提出一个更实用、效果更好的方法，就是根据该词出现的频率进行选择，相应的概率公式为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "③ 其中，f(wi)表示单词wi在单词表中出现的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. GloVe Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① GloVe算法引入了一个新的参数：\n",
    "\n",
    " - Xij: 表示i出现在j之前的次数，即i和j同时出现的次数。\n",
    " - 其中，i表示context，j表示target。\n",
    "\n",
    "② 一般地，如果不限定context一定在target的前面，则有对称关系Xij=Xji；如果有限定先后，则Xij≠Xji。\n",
    "\n",
    "③ 接下来的讨论中，我们默认存在对称关系Xij=Xji。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① GloVe模型的loss function为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "② 从上式可以看出，若两个词的embedding vector越相近，同时出现的次数越多，则对应的loss越小。\n",
    "\n",
    "③ 为了防止出现“log 0”，即两个单词不会同时出现，无相关性的情况，对loss function引入一个权重因子f(Xij)："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "④ 当Xij=0时，权重因子f(Xij)=0。这种做法直接忽略了无任何相关性的context和target，只考虑Xij>0的情况。\n",
    "\n",
    "⑤ 出现频率较大的单词相应的权重因子f(Xij)较大，出现频率较小的单词相应的权重因子f(Xij)较小一些。\n",
    "\n",
    "⑥ 具体的权重因子f(Xij)选取方法可查阅相关论文资料。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 一般地，引入偏移量，则loss function表达式为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "② 值得注意的是，参数θi和ej是对称的。\n",
    "\n",
    "③ 使用优化算法得到所有参数之后，最终的ew可表示为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "③ 最后提一点的是，无论使用Skip-Gram模型还是GloVe模型等等，计算得到的embedding matrix E的每一个特征值不一定对应有实际物理意义的特征值，如gender，age等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 情感分类一般是根据一句话来判断其喜爱程度，例如1～5星分布，如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "② 情感分类问题的一个主要挑战是缺少足够多的训练样本。\n",
    "\n",
    "③ 而Word embedding恰恰可以帮助解决训练样本不足的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 首先介绍使用word embedding解决情感分类问题的一个简单模型算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "② 如上图所示，这句话的4个单词分别用embedding vector表示。\n",
    "\n",
    "③ e8928, e2468, e4694, e3180计算均值，这样得到的平均向量的维度仍是300。\n",
    "\n",
    "④ 最后经过softmax输出1～5星。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 这种模型结构简单，计算量不大，不论句子长度多长，都使用平均的方式得到300D的embedding vector。该模型实际表现较好。\n",
    "\n",
    "② 但是，这种简单模型的缺点是使用平均方法，没有考虑句子中单词出现的次序，忽略其位置信息。而有时候，不同单词出现的次序直接决定了句意，即情感分类的结果。例如下面这句话：\n",
    "\n",
    " - Completely lacking in good taste, good service, and good ambience.\n",
    " \n",
    "③ 虽然这句话中包含了3个“good”，但是其前面出现了“lacking”，很明显这句话句意是negative的。如果使用上面介绍的平均算法，则很可能会错误识别为positive的，因为忽略了单词出现的次序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 为了解决这一问题，情感分类的另一种模型是RNN。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "② 该RNN模型是典型的many-to-one模型，考虑单词出现的次序，能够有效识别句子表达的真实情感。\n",
    "\n",
    "③ 值得一提的是使用word embedding，能够有效提高模型的泛化能力，即使训练样本不多，也能保证模型有不错的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Debiasing Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① Word embeddings中存在一些性别、宗教、种族等偏见或者歧视，例如下面这两句话：\n",
    "\n",
    " - Man: Woman as King: Queen\n",
    "\n",
    " - Man: Computer programmer as Woman: Homemaker\n",
    "\n",
    " - Father: Doctor as Mother: Nurse\n",
    "\n",
    "② 很明显，第二句话和第三句话存在性别偏见，因为Woman和Mother也可以是Computer programmer和Doctor。\n",
    "\n",
    "③ 以性别偏见为例，我们来探讨下如何消除word embeddings中偏见。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 首先，确定偏见bias的方向。方法是对所有性别对立的单词求差值，再平均。上图展示了bias direction和non-bias direction。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "② 然后，单词中立化（Neutralize）。将需要消除性别偏见的单词投影到non-bias direction上去，消除bias维度，例如babysitter，doctor等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "③ 最后，均衡对（Equalize pairs）。让性别对立单词与上面的中立词距离相等，具有同样的相似度。例如让grandmother和grandfather与babysitter的距离同一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "④ 值得注意的是，掌握哪些单词需要中立化非常重要。一般来说，大部分英文单词，例如职业、身份等都需要中立化，消除embedding vector中性别这一维度的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n### 🌍 实际应用场景\n\n1. **手机拍照优化**\n   - 场景: iPhone、华为等手机的AI拍照\n   - 应用: 自动识别场景(人像/风景/美食),优化参数\n   - 效果: 随手一拍就是大片\n   - 案例: 华为P系列手机的AI摄影获得多项大奖\n\n2. **智能家居**\n   - 场景: 小米、华为的智能家居系统\n   - 应用: 学习你的生活习惯,自动调节空调、灯光\n   - 效果: 回家前自动开空调,睡觉时自动关灯\n   - 案例: 小米IoT平台连接设备超5亿台\n\n3. **在线教育**\n   - 场景: 作业帮、猿辅导的拍照搜题\n   - 应用: 拍下题目,AI自动识别并给出解答\n   - 效果: 秒出答案和详细解析\n   - 案例: 作业帮月活用户超1.7亿\n\n4. **健康监测**\n   - 场景: Apple Watch、小米手环的健康监测\n   - 应用: 分析心率、睡眠、运动数据\n   - 效果: 及时发现健康异常,提醒就医\n   - 案例: Apple Watch曾多次救人性命\n\n5. **智能翻译**\n   - 场景: 有道翻译、Google翻译\n   - 应用: 实时翻译语音和文字\n   - 效果: 出国旅游不再担心语言不通\n   - 案例: 有道翻译支持100+种语言互译\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📚 本节小结\n",
    "\n",
    "恭喜你完成了本节学习!让我们回顾一下:\n",
    "\n",
    "### ✅ 你学到了什么?\n",
    "\n",
    "- 请在这里写下你的收获...\n",
    "\n",
    "### 🤔 还有疑问?\n",
    "\n",
    "- 请记录下你不理解的地方...\n",
    "\n",
    "### 🚀 下一步\n",
    "\n",
    "- 继续学习相关主题\n",
    "- 尝试做一些练习题\n",
    "- 应用到实际项目中\n",
    "\n",
    "---\n",
    "\n",
    "**记住:** 学习是一个循序渐进的过程,不要着急,慢慢来! 💪\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.3",
   "language": "python",
   "name": "python3.6.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "357.344px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}