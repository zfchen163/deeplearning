# CV-main æ·±åº¦å­¦ä¹ è¯¾ç¨‹ - å®Œæ•´åˆ†æä¸ç”Ÿäº§çº§ä»£ç ç¤ºä¾‹

## ğŸ“š é¡¹ç›®æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªè¶…çº§å…¨é¢çš„æ·±åº¦å­¦ä¹ ç¬”è®°åº“ï¼ŒåŒ…å« **157ä¸ª Jupyter Notebook**ï¼Œæ¶µç›–äº†ï¼š
- PyTorch åŸºç¡€ï¼ˆ100-122ï¼‰
- æ·±åº¦å­¦ä¹ ç†è®ºä¸å®è·µï¼ˆ200-268ï¼‰
- å´æ©è¾¾æ·±åº¦å­¦ä¹ ä¸“é¡¹ï¼ˆ300-354ï¼‰
- å¤§æ¨¡å‹ä¸Agentï¼ˆ400-409+ï¼‰

---

## ğŸ“‘ è¯¾ç¨‹ç»“æ„è¯¦è§£

### **ç¬¬ä¸€é˜¶æ®µï¼šPyTorch åŸºç¡€ï¼ˆ100-122ï¼‰**

#### æ ¸å¿ƒå†…å®¹ï¼š
| ç¼–å· | ä¸»é¢˜ | ç›®æ ‡ |
|------|------|------|
| 100 | é…ç½®ç‰ˆæœ¬ | ç¯å¢ƒé…ç½®ä¸ç‰ˆæœ¬ç®¡ç† |
| 101 | PyTorchå®‰è£… | æ­£ç¡®å®‰è£…PyTorchåŠCUDA |
| 102 | Pythonä¸¤å¤§æ³•å® | æŒæ¡ `dir()` å’Œ `help()` æ¢ç´¢æ¨¡å— |
| 103 | åŠ è½½æ•°æ® | æ•°æ®å¯¼å…¥ä¸é¢„å¤„ç† |
| 104 | Tensorboard | è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ– |
| 105 | Transforms | æ•°æ®å¢å¼ºå˜æ¢ |
| 106 | torchvisionæ•°æ®é›† | ä½¿ç”¨å…¬å¼€æ•°æ®é›† |
| 107 | DataLoader | æ‰¹é‡åŠ è½½ä¸å¹¶è¡ŒåŒ– |
| 108 | nn.Module | PyTorchæ¨¡å‹åŸºç±» |
| 109-113 | å„å±‚è¯¦è§£ | å·ç§¯ã€æ± åŒ–ã€æ¿€æ´»ã€çº¿æ€§å±‚ |
| 115-116 | æŸå¤±ä¸ä¼˜åŒ– | æŸå¤±å‡½æ•°ä¸åå‘ä¼ æ’­ |
| 117-118 | æ¨¡å‹ç®¡ç† | æ¨¡å‹ä¿å­˜ä¸åŠ è½½ |
| 119-121 | å®Œæ•´è®­ç»ƒ/éªŒè¯ | ç«¯åˆ°ç«¯çš„è®­ç»ƒæµç¨‹ |
| 122 | å¼€æºé¡¹ç›® | å­¦ä¹ ä¸šç•Œé¡¹ç›® |

#### ç”Ÿäº§çº§ä»£ç ç¤ºä¾‹ï¼š

```python
# 1. é…ç½®ä¸åˆå§‹åŒ–
from production_code_examples import ConfigPyTorch, setup_logger

config = ConfigPyTorch(device='cuda', seed=42)
config.setup()
logger = setup_logger('Training', 'training.log')

# 2. æ¢ç´¢æ¨¡å—
from production_code_examples import PythonMagicMethods

magic = PythonMagicMethods()
torch_attrs = magic.explore_module(torch, pattern='cuda')
print(magic.get_documentation(torch.nn, 'Linear'))

# 3. è‡ªå®šä¹‰æ•°æ®é›†
from production_code_examples import CustomDataset
from torch.utils.data import DataLoader

X, y = np.random.randn(1000, 28, 28), np.random.randint(0, 10, 1000)
dataset = CustomDataset(X, y)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)

# 4. å®Œæ•´çš„è®­ç»ƒå¾ªç¯
from production_code_examples import ResNet18Classifier, Trainer, AdvancedOptimizer

model = ResNet18Classifier(num_classes=10)
optimizer = AdvancedOptimizer.get_optimizer(model, 'adam', lr=1e-3)
trainer = Trainer(model, device='cuda')
trainer.train(train_loader, val_loader, num_epochs=100, optimizer=optimizer)

# 5. æ¨¡å‹ç®¡ç†
from production_code_examples import ModelCheckpoint

checkpoint_manager = ModelCheckpoint('./checkpoints', best_metric='val_loss')
checkpoint_manager.save_model(model, optimizer, epoch=10, 
                             metrics={'val_loss': 0.05, 'val_acc': 0.99})
```

---

### **ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦å­¦ä¹ ç†è®ºï¼ˆ200-268ï¼‰**

#### æ ¸å¿ƒä¸»é¢˜ï¼š

```
200-208: æ·±åº¦å­¦ä¹ åŸºç¡€
â”œâ”€â”€ æ•°æ®æ“ä½œä¸é¢„å¤„ç†
â”œâ”€â”€ çº¿æ€§ä»£æ•°ä¸çŸ©é˜µè®¡ç®—
â”œâ”€â”€ çº¿æ€§å›å½’ä¸ä¼˜åŒ–ç®—æ³•
â”œâ”€â”€ Softmaxå›å½’ä¸åˆ†ç±»
â””â”€â”€ å¤šå±‚æ„ŸçŸ¥æœº

210-226: å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰
â”œâ”€â”€ æƒé‡è¡°é€€ä¸æ­£åˆ™åŒ–
â”œâ”€â”€ ä¸¢å¼ƒæ³•ï¼ˆDropoutï¼‰
â”œâ”€â”€ å·ç§¯å±‚åŸç†
â”œâ”€â”€ æ± åŒ–å±‚
â”œâ”€â”€ ç»å…¸æ¶æ„ï¼ˆLeNet, AlexNet, VGG, NiN, GoogLeNet, ResNetï¼‰
â””â”€â”€ æ‰¹é‡å½’ä¸€åŒ–

228-240: ç›®æ ‡æ£€æµ‹
â”œâ”€â”€ ç¡¬ä»¶é…ç½®ï¼ˆTPUç­‰ï¼‰
â”œâ”€â”€ å¤šGPUè®­ç»ƒ
â”œâ”€â”€ æ•°æ®å¢å¹¿
â”œâ”€â”€ å¾®è°ƒ
â”œâ”€â”€ ç‰©ä½“æ£€æµ‹ç®—æ³•ï¼ˆR-CNN, SSD, YOLOï¼‰

241-265: è¯­ä¹‰åˆ†å‰²ä¸NLP
â”œâ”€â”€ è¯­ä¹‰åˆ†å‰²
â”œâ”€â”€ è½¬ç½®å·ç§¯
â”œâ”€â”€ å…¨è¿æ¥å·ç§¯ç½‘ç»œï¼ˆFCNï¼‰
â”œâ”€â”€ æ ·å¼è¿ç§»
â”œâ”€â”€ åºåˆ—æ¨¡å‹ä¸RNN
â”œâ”€â”€ å¾ªç¯ç¥ç»ç½‘ç»œå˜ç§ï¼ˆGRU, LSTMï¼‰
â”œâ”€â”€ ç¼–ç å™¨-è§£ç å™¨æ¶æ„
â”œâ”€â”€ Seq2seqä¸æ³¨æ„åŠ›æœºåˆ¶
â””â”€â”€ Transformerä¸BERT
```

#### å…³é”®ç®—æ³•çš„ç”Ÿäº§çº§å®ç°ï¼š

```python
# 1. æ‰¹é‡å½’ä¸€åŒ–
from production_code_examples import BatchNormalization

x = torch.randn(32, 64)
gamma = torch.ones(64)
beta = torch.zeros(64)
normalized, cache, running_stats = BatchNormalization.batch_norm_1d(
    x, gamma, beta, running_mean, running_var, training=True
)

# 2. é«˜çº§æŸå¤±å‡½æ•°
from production_code_examples import LossFunctions

loss_fn = LossFunctions.get_loss_function(task_type='classification')

# 3. å­¦ä¹ ç‡è°ƒåº¦
scheduler = AdvancedOptimizer.get_scheduler(optimizer, 'cosine', num_epochs=100)

# 4. æ¨¡å‹åˆ†æ
from production_code_examples import ModelAnalyzer

total_params, trainable_params = ModelAnalyzer.count_parameters(model)
ModelAnalyzer.print_model_summary(model, input_size=(1, 224, 224, 3))
```

---

### **ç¬¬ä¸‰é˜¶æ®µï¼šå´æ©è¾¾æ·±åº¦å­¦ä¹ ä¸“é¡¹ï¼ˆ300-354ï¼‰**

#### è¯¾ç¨‹è¦†ç›–ï¼š

```
è¯¾ç¨‹1ï¼šç¥ç»ç½‘ç»œåŸºç¡€ï¼ˆ301-309ï¼‰
â”œâ”€â”€ æ·±åº¦å­¦ä¹ æ¦‚è¿°
â”œâ”€â”€ ç¥ç»ç½‘ç»œåŸºç¡€
â”œâ”€â”€ Pythonä¸å‘é‡åŒ–
â”œâ”€â”€ æµ…å±‚ç¥ç»ç½‘ç»œ
â””â”€â”€ æ·±å±‚ç¥ç»ç½‘ç»œ

è¯¾ç¨‹2ï¼šæ”¹è¿›æ·±å±‚ç¥ç»ç½‘ç»œï¼ˆ314-321ï¼‰
â”œâ”€â”€ å®ç”¨å±‚é¢
â”œâ”€â”€ ä¼˜åŒ–ç®—æ³•ï¼ˆMomentum, RMSprop, Adamï¼‰
â”œâ”€â”€ è¶…å‚æ•°è°ƒè¯•
â””â”€â”€ Batchæ­£åˆ™åŒ–

è¯¾ç¨‹3ï¼šæœºå™¨å­¦ä¹ ç­–ç•¥ï¼ˆ323-328ï¼‰
â”œâ”€â”€ æœºå™¨å­¦ä¹ ç­–ç•¥ï¼ˆä¸Šï¼‰
â””â”€â”€ æœºå™¨å­¦ä¹ ç­–ç•¥ï¼ˆä¸‹ï¼‰

è¯¾ç¨‹4ï¼šå·ç§¯ç¥ç»ç½‘ç»œï¼ˆ329-341ï¼‰
â”œâ”€â”€ CNNåŸºç¡€
â”œâ”€â”€ ç›®æ ‡æ£€æµ‹ï¼ˆYoloç­‰ï¼‰
â”œâ”€â”€ äººè„¸è¯†åˆ«ä¸é£æ ¼è¿ç§»

è¯¾ç¨‹5ï¼šå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆ342-353ï¼‰
â”œâ”€â”€ RNNåŸºç¡€
â”œâ”€â”€ ç‰¹å¾å‘é‡è¡¨å¾ï¼ˆWord2Vec, GloVeï¼‰
â”œâ”€â”€ åºåˆ—æ¨¡å‹ä¸æ³¨æ„åŠ›æœºåˆ¶
â””â”€â”€ å®æˆ˜é¡¹ç›®ï¼ˆRNN, LSTM, æœºå™¨ç¿»è¯‘ï¼‰
```

#### é«˜çº§ä¼˜åŒ–æŠ€æœ¯å®ç°ï¼š

```python
# 1. å¤šç§ä¼˜åŒ–å™¨
optimizers = {
    'SGD': optim.SGD(model.parameters(), lr=1e-3, momentum=0.9),
    'Adam': optim.Adam(model.parameters(), lr=1e-3),
    'RMSprop': optim.RMSprop(model.parameters(), lr=1e-3),
    'AdamW': optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)
}

# 2. æ—©åœæœºåˆ¶
from production_code_examples import EarlyStopping

early_stopping = EarlyStopping(patience=10, min_delta=1e-4)
for epoch in range(num_epochs):
    val_loss = train_one_epoch()
    if early_stopping(val_loss):
        break

# 3. å­¦ä¹ ç‡é¢„çƒ­
from torch.optim.lr_scheduler import LinearLR

scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=5)
```

---

### **ç¬¬å››é˜¶æ®µï¼šå¤§æ¨¡å‹ä¸Agentï¼ˆ400-409+ï¼‰**

#### ä¸»é¢˜è¦†ç›–ï¼š

```
402: å‘é‡æ•°æ®åº“ï¼ˆVector DBï¼‰
     â”œâ”€â”€ Embedding ç”Ÿæˆ
     â”œâ”€â”€ ç›¸ä¼¼åº¦æœç´¢
     â””â”€â”€ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰

404: ä»£ç åˆ†æä¸å±•æœ›

406-407: Pythonå¤§æ¨¡å‹
     â”œâ”€â”€ æ‰‹å†™å®ç°
     â””â”€â”€ APIç¯å¢ƒé…ç½®

409: å¤šè½®å¯¹è¯ç³»ç»Ÿ
     â”œâ”€â”€ å¯¹è¯çŠ¶æ€ç®¡ç†
     â”œâ”€â”€ ä¸Šä¸‹æ–‡ç†è§£
     â””â”€â”€ é•¿æœŸè®°å¿†
```

#### ç”Ÿäº§çº§Agentå®ç°ï¼š

```python
from production_code_examples import Agent, ConversationMemory

# 1. åˆå§‹åŒ–Agent
agent = Agent(
    model_name="gpt-4",
    system_prompt="You are a helpful AI assistant specialized in deep learning."
)

# 2. æ³¨å†Œå·¥å…·
def calculate_sum(a, b):
    return a + b

agent.register_tool("calculate_sum", calculate_sum)

# 3. å¤šè½®å¯¹è¯
agent.process_input("What is neural networks?")
agent.process_input("Can you explain backpropagation?")

# 4. å¯¹è¯å†å²ç®¡ç†
context = agent.memory.get_conversation_context()
agent.memory.save('./conversation.json')
agent.memory.load('./conversation.json')
```

---

## ğŸ¯ ç”Ÿäº§çº§ä»£ç æ¡†æ¶

### 1. **å®Œæ•´çš„è®­ç»ƒæ¡†æ¶**

```python
from production_code_examples import Trainer, ModelCheckpoint, EarlyStopping

# é…ç½®
config = ConfigPyTorch()
config.setup()

# åˆ›å»ºæ¨¡å‹
model = ResNet18Classifier(num_classes=10)

# ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
optimizer = AdvancedOptimizer.get_optimizer(model, 'adamw', lr=1e-3)
scheduler = AdvancedOptimizer.get_scheduler(optimizer, 'cosine', num_epochs=100)

# æŸå¤±å‡½æ•°
loss_fn = LossFunctions.get_loss_function('classification')

# åˆ›å»ºè®­ç»ƒå™¨
trainer = Trainer(model, device=config.device, log_dir='./logs')

# æ¨¡å‹æ£€æŸ¥ç‚¹
checkpoint_manager = ModelCheckpoint('./checkpoints', best_metric='val_loss')

# è®­ç»ƒ
trainer.train(
    train_loader=train_loader,
    val_loader=val_loader,
    num_epochs=100,
    optimizer=optimizer,
    loss_fn=loss_fn,
    scheduler=scheduler,
    checkpoint_manager=checkpoint_manager
)
```

### 2. **æ—¥å¿—ä¸ç›‘æ§**

```python
from production_code_examples import setup_logger

# åˆ›å»ºlogger
logger = setup_logger('DeepLearning', 'training.log', level=logging.INFO)

# ä½¿ç”¨logger
logger.info("Training started")
logger.warning("Learning rate decreased")
logger.error("CUDA out of memory")

# TensorBoardé›†æˆ
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/experiment1')
writer.add_scalar('Loss/train', loss, epoch)
writer.add_scalar('Accuracy/train', acc, epoch)
writer.close()
```

### 3. **æ¨¡å‹ä¿å­˜ä¸æ¢å¤**

```python
from production_code_examples import ModelCheckpoint

checkpoint_manager = ModelCheckpoint(
    save_dir='./checkpoints',
    best_metric='val_loss'
)

# ä¿å­˜
checkpoint_manager.save_model(
    model=model,
    optimizer=optimizer,
    epoch=10,
    metrics={'val_loss': 0.05, 'val_accuracy': 0.99}
)

# åŠ è½½
checkpoint = checkpoint_manager.load_model(model, optimizer, 'checkpoints/best_model.pt')
```

### 4. **å¤šè½®å¯¹è¯ç³»ç»Ÿ**

```python
from production_code_examples import ConversationMemory, Agent

# åˆ›å»ºå¯¹è¯å†…å­˜
memory = ConversationMemory(max_history=20)

# æ·»åŠ æ¶ˆæ¯
memory.add_message('user', 'What is deep learning?')
memory.add_message('assistant', 'Deep learning is...')

# è·å–ä¸Šä¸‹æ–‡
context = memory.get_conversation_context()

# ä¿å­˜å’ŒåŠ è½½
memory.save('conversation.json')
memory.load('conversation.json')
```

---

## ğŸ“Š æ ¸å¿ƒæ¦‚å¿µé€ŸæŸ¥è¡¨

### æ•°æ®å¤„ç†
- **Transform**: æ•°æ®å¢å¼ºä¸é¢„å¤„ç†
- **DataLoader**: æ‰¹é‡åŠ è½½ä¸å¤šçº¿ç¨‹å¤„ç†
- **Dataset**: è‡ªå®šä¹‰æ•°æ®é›†

### æ¨¡å‹æ„å»º
- **nn.Module**: åŸºç±»
- **Sequential**: é¡ºåºå®¹å™¨
- **ModuleList**: æ¨¡å—åˆ—è¡¨

### è®­ç»ƒæŠ€å·§
| æŠ€å·§ | ç›®çš„ | ä½•æ—¶ä½¿ç”¨ |
|------|------|--------|
| Batch Norm | åŠ é€Ÿè®­ç»ƒï¼Œç¨³å®šæ€§ | æ‰€æœ‰ç°ä»£ç½‘ç»œ |
| Dropout | æ­£åˆ™åŒ–ï¼Œé˜²è¿‡æ‹Ÿåˆ | ç½‘ç»œè¾ƒå¤§æ—¶ |
| Weight Decay | L2æ­£åˆ™åŒ– | é˜²æ­¢æƒé‡è¿‡å¤§ |
| Learning Rate Schedule | è‡ªé€‚åº”å­¦ä¹ ç‡ | æ‰€æœ‰è®­ç»ƒ |
| Early Stopping | æå‰åœæ­¢ | éªŒè¯é›†å¼€å§‹ä¸Šå‡æ—¶ |
| Gradient Clipping | é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ | RNN, Transformers |

### ä¼˜åŒ–å™¨å¯¹æ¯”
| ä¼˜åŒ–å™¨ | é€‚ç”¨åœºæ™¯ | å­¦ä¹ ç‡ |
|--------|--------|--------|
| SGD | åŸºç¡€ä»»åŠ¡ | 1e-1 ~ 1e-3 |
| SGD + Momentum | æ ‡å‡†é€‰æ‹© | 1e-2 ~ 1e-4 |
| Adam | å¿«é€Ÿæ”¶æ•› | 1e-3 ~ 1e-5 |
| AdamW | é•¿æœŸè®­ç»ƒ | 1e-3 ~ 1e-5 |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ– venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install tensorboard numpy matplotlib pandas scikit-learn
```

### è¿è¡Œç¤ºä¾‹

```bash
cd /Users/h/practice/CV-main
python production_code_examples.py
```

### ä½¿ç”¨ç”Ÿäº§ä»£ç 

```python
from production_code_examples import *

# åˆå§‹åŒ–
config = ConfigPyTorch()
config.setup()

# åˆ›å»ºæ¨¡å‹
model = ResNet18Classifier(num_classes=10)

# è®­ç»ƒ
trainer = Trainer(model)
trainer.train(train_loader, val_loader, num_epochs=100, optimizer=optimizer)
```

---

## ğŸ“ˆ å­¦ä¹ è·¯å¾„å»ºè®®

### åˆå­¦è€…ï¼ˆ0-3ä¸ªæœˆï¼‰
1. **ç¬¬1å‘¨**: å­¦ä¹ 100-107ï¼Œç†è§£PyTorchåŸºç¡€
2. **ç¬¬2-3å‘¨**: å­¦ä¹ 108-118ï¼ŒæŒæ¡ç¥ç»ç½‘ç»œæ„å»º
3. **ç¬¬4å‘¨**: å­¦ä¹ 119-122ï¼Œå®Œæˆç¬¬ä¸€ä¸ªå®Œæ•´é¡¹ç›®

### ä¸­çº§å­¦ä¹ è€…ï¼ˆ3-6ä¸ªæœˆï¼‰
1. æ·±å…¥å­¦ä¹ 200-268ï¼Œç†è§£æ·±åº¦å­¦ä¹ ç†è®º
2. å®ç°å„ç§ç»å…¸ç½‘ç»œï¼ˆLeNet, AlexNet, VGG, ResNetï¼‰
3. å‚åŠ Kaggleç«èµ›

### é«˜çº§å­¦ä¹ è€…ï¼ˆ6-12ä¸ªæœˆï¼‰
1. å­¦ä¹ 300-354ï¼Œå´æ©è¾¾ä¸“é¡¹ç³»ç»Ÿå­¦ä¹ 
2. å­¦ä¹ æœ€æ–°çš„Transformer, BERTç­‰
3. å­¦ä¹ å¤§æ¨¡å‹å’ŒAgent

---

## ğŸ”— ç›¸å…³èµ„æº

- **å®˜æ–¹æ–‡æ¡£**: https://pytorch.org/docs/
- **å´æ©è¾¾è¯¾ç¨‹**: https://www.deeplearning.ai/
- **ææ²æ·±åº¦å­¦ä¹ **: https://github.com/d2l-ai/d2l-zh
- **TensorBoard**: https://www.tensorflow.org/tensorboard
- **Kaggleç«èµ›**: https://www.kaggle.com/

---

## ğŸ’¡ ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

### 1. **é…ç½®ç®¡ç†**
```python
@dataclass
class TrainingConfig:
    batch_size: int = 32
    num_epochs: int = 100
    learning_rate: float = 1e-3
    weight_decay: float = 1e-5
    device: str = 'cuda'
    seed: int = 42
```

### 2. **é”™è¯¯å¤„ç†**
```python
try:
    # è®­ç»ƒä»£ç 
    trainer.train(...)
except RuntimeError as e:
    logger.error(f"CUDA error: {e}")
    # å›é€€åˆ°CPU
    device = 'cpu'
except Exception as e:
    logger.error(f"Unexpected error: {e}")
    traceback.print_exc()
```

### 3. **ç‰ˆæœ¬ç®¡ç†**
```python
def save_experiment_config(config, model, path):
    """ä¿å­˜å®Œæ•´çš„å®éªŒé…ç½®"""
    experiment_info = {
        'config': asdict(config),
        'model_architecture': str(model),
        'timestamp': datetime.now().isoformat(),
        'pytorch_version': torch.__version__,
        'cuda_version': torch.version.cuda
    }
    with open(path, 'w') as f:
        json.dump(experiment_info, f, indent=2)
```

### 4. **åˆ†å¸ƒå¼è®­ç»ƒ**
```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel

dist.init_process_group(backend='nccl')
model = DistributedDataParallel(model, device_ids=[local_rank])
```

---

## ğŸ“ æ€»ç»“

è¿™ä¸ªCV-mainè¯¾ç¨‹åº“æ˜¯ä¸€ä¸ª**ç³»ç»Ÿã€å…¨é¢çš„æ·±åº¦å­¦ä¹ å­¦ä¹ èµ„æº**ï¼Œæ¶µç›–äº†ä»åŸºç¡€åˆ°å‰æ²¿çš„æ‰€æœ‰å†…å®¹ã€‚

**å…³é”®æ”¶è·**ï¼š
âœ… æŒæ¡PyTorchæ¡†æ¶  
âœ… ç†è§£æ·±åº¦å­¦ä¹ ç†è®º  
âœ… å­¦ä¼šæ„å»ºç”Ÿäº§çº§ä»£ç   
âœ… äº†è§£æœ€æ–°çš„Transformerå’Œå¤§æ¨¡å‹  
âœ… è·å¾—Agentå¼€å‘èƒ½åŠ›  

**ä¸‹ä¸€æ­¥**ï¼š
1. é€ä¸ªå­¦ä¹ æ¯ä¸ªNotebook
2. ä½¿ç”¨ç”Ÿäº§ä»£ç æ¡†æ¶å®ç°ç»ƒä¹ 
3. å‚åŠ Kaggleç«èµ›æˆ–çœŸå®é¡¹ç›®
4. è´¡çŒ®å›é¦ˆç¤¾åŒº

---

**ç”Ÿæˆæ—¶é—´**: 2026-01-27  
**ä½œè€…**: AI Assistant  
**ç‰ˆæœ¬**: 1.0
